\documentclass{article}
\usepackage[sorting=none]{biblatex}
\addbibresource{references.bib}

\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,verbatim}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmicx}

\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[bb=boondox]{mathalfa}
\usepackage{enumerate}
\usepackage{xcolor}

\renewcommand{\algorithmiccomment}[1]{{\footnotesize\hfill$\triangleright$ #1}}

\renewcommand{\P}{\mathbf{P}}
\newcommand{\NP}{\mathbf{NP}}
\newcommand{\coNP}{\mathbf{coNP}}
\newcommand{\EXP}{\mathbf{EXP}}
\newcommand{\BPP}{\mathbf{BPP}}
\newcommand{\RP}{\mathbf{RP}}
\newcommand{\NEXP}{\mathbf{NEXP}}
\newcommand{\PH}{\mathbf{PH}}
\newcommand{\PSPACE}{\mathbf{PSPACE}}
\newcommand{\TIME}{\mathbf{TIME}}
\newcommand{\NTIME}{\mathbf{NTIME}}
\newcommand{\LOG}{\mathbf{LOGSPACE}}
\newcommand{\SIZE}{\mathbf{SIZE}}

\def \F {{\mathbb F}}
\def \N {{\mathbb N}}

\def \ATIME{{\mathsf{ATIME}}}
\def \NTIME{{\mathsf{NTIME}}}
\def \eps {{\varepsilon}}

\def \ASPACE{{\mathsf{ASPACE}}}
\def \SPACE{{\mathsf{SPACE}}}
\def \TIME{{\mathsf{TIME}}}
\def \BPL{{\mathbf{BPL}}}

\def \poly{\text{poly}}

\def \Ygood{\mathcal{Y}_\text{good}}
\def \Rgood{\mathcal{R}_\text{good}}
\def \by{{\bar{y}}}
\def \PP{\P\P}

\theoremstyle{definition}

\newtheorem{defn}{Definition}
\newtheorem*{defn*}{Definition}
\newtheorem*{notn*}{Notation}

\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{prop*}{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem*{cor*}{Corollary}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}
\newtheorem*{recall}{Recall}

\def \Pa{\text{Pa}}

\begin{document}

\begin{center}
    \Large
    % Are there Bayesian networks in which posterior inference is difficult on typical observations?
    % On the complexity of posterior inference under typical-case observations in worst-case Bayesian networks
    Is there an efficient algorithm for approximate posterior inference in all Bayesian networks on typical-case observations?

    \vspace{3pt}
    \normalsize
    George Matheos \quad \vline \quad Final project report for MIT course 6.541 \quad \vline \quad \today

    % \vspace{3pt}
    % Draft as of \today
\end{center}

\begin{abstract}
\noindent In this paper, I extend Dagum and Luby's 1993 result \cite{dagum1993}, strengthening the sense in which it is known that under standard complexity-theoretic conjectures, there are no efficient general-purpose algorithms for approximate posterior inference in Bayesian networks.
Dagum and Luby showed that if $\P \neq \NP$, then there is no algorithm which can do approximate posterior inference in any Bayesian network, conditioning on worst-case observed values.
I extend this to show that if pseudorandom generators or one-way functions exist,
then there is no algorithm which can do approximate posterior inference in every Bayesian network, on random observations distributed according to the Bayesian network, with probability greater than 3/4.
Under the aformentioned assumptions, this implies that there is no general-purpose approximate inference algorithm which succeeds with very high probability when conditioning on observations generated by random processes in the world which are well described by the probabilistic model on hand.
\end{abstract}

\section{Introduction}
A central computational problem in science, artificial intelligence, and statistics, is posterior inference in probabilistic models.
This problem's input is the description of a probability distribution $P$ on random values $X$ and $Y$, and an assignment $y$ to $Y$.
Its output is a description of the posterior distribution $P(X | Y = y)$.
A natural question for complexity theorists, therefore, is how hard variants of this problem are.
One family of probabilistic models in which to study the complexity of inference is the family of Bayesian networks.
A Bayesian network is a directed acyclic graph whose nodes are random variables, and whose edges represent probabilistic dependencies between the variables.
Roughly, a Bayesian network is the probabilistic analog of a circuit (whereas a general probabilistic program \cite{mansinghka2009natively} is the probabilistic analog of a Turing machine).
% Under the strictest problem formulations, requiring the ability to compute values like $P(X = x | Y = y)$ exactly under arbitrary probabilistic programs \cite, probabilistic inference is $\#\P$ complete.
% This problem formulation is very strict relative to what is needed in practice, and as a result, theorists have studied a number of weaker formulations of the problem, such as the complexity approximate inference, and the complexity of inference in Bayesian networks, which are roughly the probabilistic analog to circuits (as opposed to general probabilistic programs, which are the probaiblistic analog of Turing machines).
In the early 1990s, Cooper \cite{cooper1990} showed that if there were a general-purpose algorithm for exact probabilistic inference in Bayesian networks, then $\P = \NP$.
Although Cooper did not note this in his paper, it follows from his result that such an algorithm would in fact imply $\P = \PP$, a stronger statement.
\footnote{
The reason for this is as follows.
% Note that this is arguably a pretty weak result, since a straightforward reduction
% shows that a general-purpose exact inference algorithm actually implies $\P = \PP$, which is a stronger statement.
The canonical problem in $\PP$ is to determine whether more than half of the inputs $y$ of an upper-bounded size to a Turing machine $M(x, y)$ cause it to output 1, for a given $x$.
For any given $x$ we can construct a Bayesian network which posits that the bitstring $y$ is generated from a uniformly random distribution, and another variable $z$ has a value exactly equal to $M(x, y) = 1$.
This involves encoding a circuit for $M$ on inputs of this length into the Bayesian network.
The $\PP$ counting problem boils down to determining if $P(z = 1) \geq 1/2$.
Say that there were indeed a general-purpose exact posterior inference algorithm for Bayesian networks.
Then by Cooper's result, $\P = \NP$, so for any value $x$ it would be possible to find a value $y$ such that $M(x, y) = 1$ in polynomial time.
Then, if it were possible to compute $P(Y = y | z = 1)$ exactly,
because $P(Y = y | z = 1) = P(Y = y, z = 1) / P(z = 1)$, and the value $P(Y = y, z = 1)$ is simply $\frac{1}{2^{|y|}}$ in this case,
an algorithm could compute $P(z = 1)$ exactly in polynomial time.
This would allow it to solve the counting problem in $\PP$ in polynomial time.
}
In 1993, noting that exact computation of posterior distributions is often unnecessary, and computing approximations is often sufficient, Dagum and Luby \cite{dagum1993} extended Cooper's result to to show that in a strong sense, a general purpose algorithm for approximate posterior inference in Bayesian networks would also imply $\P = \NP$.

These proofs work by showing that for any instance of an NP-hard problem, there exists a Bayesian network $P$ and an observation $y$ such that computing the posterior $P(X = \cdot | Y = y)$ would yield a solution to the NP-hard problem.
Just as exact inference is often unnecessary in practice, the ability to do inference under any worst-case observed value $y$ is also unnecessary in many settings, and hence the worst-case nature of the value $y$ in these constructions is unsatisfying.

Frequently, the probability distributions $P$ under which it is relevant to do inference in practice are those which to some extent describe the distribution of things occurring in the world.
Thus in many settings, if the model $P$ is well-specified, the observed values $y$ which will present themselves and under which we will need to do inference will be generated by a process in the world with a similar distribution to $P$.
In such a situation, if one needed to do inference under $P$ but there were a set $\mathcal{Y}_\text{hard}$ of observations $y$ under which inference were computationally difficult, this would probably not be a problem if it were the case that $P(\mathcal{Y}_\text{hard}) < 0.0001$.
Dagum and Luby's hardness results therefore leave open the possibility that there exist general-purpose inference algorithms which succeed on typical-case observations, so long as the probabilistic model in question is well-specified (that is, matches the distribution of observations in the world).
% Despite interpretations of these hardness results claiming that they rule out useful general-purpose inference algorithms, it is not clear that this is the case.
If it were possible to develop an inference algorithm which almost always worked in any well-specified Bayesian network model, that would be extremely valuable.
This possibility has not been ruled out by existing results on the complexity of probabilistic inference.

In this paper, I extend Dagum and Luby's 1993 result \cite{dagum1993} to show that not only is approximate inference hard on worst-case observations, but in fact, if standard complexity theoretic conjectures hold, then there exist Bayesian networks in which inference under a randomly-generated observation is hard with probability 1/4.
That is, if these standard conjectures hold, then for any inference algorithm $A$, there exists a Bayesian network $P$ and a set of observations $\mathcal{Y}_\text{hard}$ such that $P(\mathcal{Y}_\text{hard}) \geq 1/4$, and the inference algorithm fails approximately compute $P(X | Y = y)$ on any $y \in \mathcal{Y}_\text{hard}$.
Theorem~\ref{thm:main} shows that one sufficient condition for this result is the existence of a pseudorandom generator.
% Specifically, Theorem~\ref{thm:main} shows that if pseudo-random generators exist, there is no algorithm which can do inference on at least 1/4 of observations in any Bayesian networks.
From this follows Corollary~\ref{cor:oneway}, stating that the existence of one-way functions also imply the nonexistence of universal inference algorithms which succeed on a $3/4$-typical set of observations.
Roughly, this means that if general-purpose typical-case approximate inference is possible, then public key cryptography is impossible.

This result provides new evidence that there are no general-purpose inference algorithms which succeed in every probabilistic model.
However, note that the notion of typicality here is only with respect to observations in a given worst-case model.
The possibility remains that there are meaningful senses in which for typical probabilistic models, inference is computationally easy.

\subsection{Summary of the proof of the main result}
The key idea in the proof of Theorem~\ref{thm:main} is of a fairly different character from the reductions used in proofs of the hardness of inference from Cooper \cite{cooper1990} and Dagum and Luby \cite{dagum1993}.
The starting point for this proof is not the assumption that $\P \neq \NP$, but instead the assumption of the existence of a pseudorandom generator.
This is a function $G: \{0, 1\}^* \to \{0, 1\}^*$ such that for any polynomial-time algorithm $A$, the distribution of $A$'s outputs on uniformly random inputs is very close to the distribution of $A$'s outputs on the outputs of $G$.
% Speaking coarsely, one implication of this is that no polynomial-time algorithm can distinguish the outputs of $G$ from uniformly random strings with nontrivial probability.

The proof of Theorem~\ref{thm:main} involves the construction of a Bayesian network such that inference under typical case observations would allow one to distinguish the outputs of the pseudorandom generator from random strings.
The Bayesian network implements a probabilistic process which first randomly decides whether to output a uniformly random bitstring, or to output a value computed by the pseudorandom generator $G$.
Let $x$ be a bit indicating which branch was chosen,
let $y$ denote the output of the process,
and consider the posterior distribution $P(x = 1 | y)$.
This gives the posterior probability that the value $y$ was generated by the pseudorandom generator, as opposed to uniformly at random.
Under the specific Bayesian network constructed in my proof, I show that it cannot be easy to compute $P(x = 1 | y)$ up to $\epsilon$ additive error for any $\epsilon < 1/2$ on a large typical set of $y$ values.
Specifically, I show that if there is an algorithm $A$ and a set of bitstrings
$\Ygood$ with more than 3/4 probability under the generative process $P$,
ie. if 
$P(\Ygood) > 3/4$,
and $A$ computes an $\epsilon$-approximation of $P(x = 1 | y)$
for all $y \in \Ygood$,
then $A$ distinguishes the outputs of $G$ from random strings with nontrivial probability, in such a way that the distribution of the output of $A$ on random bitstrings differs noticably from the distribution of outputs of $A$ on bitstrings output by $G$.
If $A$ runs in polynomial time, this contradicts the assumption that $G$ is a pseudorandom generator.

\subsection{Paper outline}
Section~\ref{sec:posterior_inference} defines the problem of posterior inference and several computational formulations of it.
Subsection~\ref{sec:hardness_relationships} describes several hardness relationships between these formulations.
These relationships indicate that if it is computationally hard to perform a particular form of approximate probabilistic inference, called additive-error probability mass function approximation, then it is computationally difficult to perform probabilistic inference per any of the other standard formulations.
Section~\ref{sec:background} provides background on Bayesian networks and inference problems.
In this section I define the problem of typical-case PMF approximation up to additive error.
It follows from subsection~\ref{sec:hardness_relationships} that if it is this is a computationally hard problem, inference on typical-case observations is hard under any of the other formulations outlined herein.
Section~\ref{sec:background} also states Dagum and Luby's 1993 hardness result, and the conjectures of the existence of one-way functions and secure pseudorandom generators.
Section~\ref{sec:main} states the main result of this paper, Theorem~\ref{thm:main}, which shows that if secure pseudorandom generators exist, then there is no algorithm nor probabilistic algorithm which, for any Bayesian network, can perform additive PMF approximation, up to error $\epsilon$ for any $\epsilon < 1/2$, on a $(1 - \rho)$-typical set of observations, for any $\rho < 1/4$.

\section{Posterior inference and computation} \label{sec:posterior_inference}

\subsection{Posterior inference}

Consider a probability distribution $P$ on a space $X \times Y \times Z$.
For the purposes of this paper, we will take $X, Y, Z \subseteq \{0, 1\}^*$.
The problem of posterior inference is to characterize the posterior distribution
$$
P(X = \cdot | Y = y)
$$
for a value $y \in Y$ s.t. $P(Y = y) > 0$.

I first define the marginal distributions
$$
P(X = x, Y = y) := \sum_{z \in Z} P(x, y, z); \quad P(Y = y) := \sum_{(x, z) \in X \times Z} P(x, y, z)
$$
The posterior $P(X = \cdot | Y = y)$ is the probability distribution such that for all $x \in X$,
$$
P(X = x | Y = y) := \frac{P(X = x, Y = y)}{P(Y = y)}
$$

Stated informally, the problem of posterior inference is:
\textit{given a probabilistic model $P$ and a value $y$ with $P(Y = y) > 0$, characerize $P(X = x | Y = y)$}.

Representing a probability distribution involves representing real-valued quantities, raising the question of how these values are stored in a computer.
In this paper, I assume all real values are represented in computers as rational numbers stored exactly using a finite number of bits.
The results in this paper are independent from the exact encoding, so I will not specify the representation of real values further.

\subsection{Computational formulations of exact inference}
% The posterior $P(X = \cdot | Y = y)$ is a probability distribution on the space $X$.
% While it is desirable to have algorithms which can characterize these posteriors, there is no one canonical representation of a probability distribution for an algorithm to output.
% Hence, there are several distinct formulations of what it means for an algorithm
To formalize the problem of posterior inference as a specification a computer algorithm could satisfy, we must specify what it would mean for an algorithm to ``characterize'' a probability distribution on the space $X$, as this is the type of mathematical object that $P(X = x | Y = y)$ is.
There are several distinct algorithmic formulations:
\begin{enumerate}
    \item \textbf{Probability mass function (PMF) computation.} Given a value $x \in X, y \in Y$, compute the number $P(X = x | Y = y)$.
    \item \textbf{Cumulative distribution function (CDF) computation.} Given a value $x \in X, y \in Y$, and letting strings in $x \in X \subseteq \{0, 1\}^*$ have lexographic ordering, compute the number $P(X \leq x | Y = y)$.
    \item \textbf{Sampling.} Given a value $y \in Y$, sample a random value in $X$ according to the distribution $P(X = x | Y = y)$.  This requires the use of a probabilistic Turing machine.  Under standard formulations, this means we want a Turing machine $A$ which, given $y \in Y$ and an uniformly random bit sequence $r \in \{0, 1\}^*$, computes a value $A(y, r) \in X$ such that for all $x \in X$, $P(X = x | Y = y) = \Pr_r[x = A(y, r)]$.\footnote{
        The definition of exact sampling is more complicated when there are probabilities which do not have the form $m/2^k$ for some $m, k \in \mathbb{N}$.
        For the purposes of this paper on approximate inference, this can be ignored.
    }
    % \item \textbf{Expected value computation.} Given a value $y \in Y$ and a function $f : X \to \mathbb{R}$, compute the expected value $\mathbb{E}_{x \sim P(X = \cdot | Y = y)}[f(x)]$.  To make this formulation precise, we need a manner in which the function $f$ is represented using a bit string which can be input to an algorithm, and we need to specify how the outputs of $f$ are represented as bit strings.  Due to this complexity, the complexity theory literature appears to 
\end{enumerate}

% There are a number of reductions among these:
% \begin{enumerate}
%     \item If algorithm $A$ performs CDF computation, then it is possible to perform PMF computation by computing $P(X = x | Y = y) = \textsf{CDF}(x, y) - \textsf{CDF}(x - 1, y)$.
%     \item If algorithm $A$ performs sampling, 
% \end{enumerate}

\subsection{Computational formulations of approximate inference}
It usually suffices for algorithms to perform approximate probabilistic inference.
There are several standard formulations of approximate probabilistic inference as computational problems:
\begin{enumerate}
    \item \textbf{PMF evalution up to additive error.} Fix $\epsilon > 0$.  Algorithm $A$ does PMF evaluation in $P$ up to additive error $\epsilon$ if for any $x \in X, y \in Y$, it computes a number $A(x, y)$ where $|A(x, y) - P(X = x | Y = y)| < \epsilon$.
    \item \textbf{PMF evaluation up to relative error.} Fix $\epsilon > 0$.  Algorithm $A$ does PMF evaluation in $P$ up to relative error $\epsilon$ if for any $x \in X, y \in Y$, it computes a number $A(x, y) \in (\frac{1}{1 + \epsilon}P(X = x | Y = y), (1 + \epsilon)P(X = x | Y = y))$
    \item \textbf{CDF evaluation up to additive error.} Fix $\epsilon > 0$.  Algorithm $A$ does CDF evaluation in $P$ up to additive error $\epsilon$ if for any $x \in X, y \in Y$, it computes a number $A(x, y)$ where $|A(x, y) - P(X \leq x | Y = y)| < \epsilon$.
    \item \textbf{CDF evaluation up to relative error.} Fix $\epsilon > 0$.  Algorithm $A$ does CDF evaluation in $P$ up to relative error $\epsilon$ if for any $x \in X, y \in Y$, it computes a number $A(x, y) \in (\frac{1}{1 + \epsilon}P(X \leq x | Y = y), (1 + \epsilon)P(X \leq x | Y = y))$.
    \item \textbf{Approximate sampling with pointwise probability bounds.} There are several formulations of approximate sampling. One formulation is to say that probabilistic algorithm $A(y, r)$ (where the probabilistic nature of the algorithm is derived from its consumption of a uniformly random bitstring $r$) approximately samples from $P(X = x | Y = y)$ up to pointwise error $\delta$ if for all $x \in X$, $|\Pr_r[A(y, r) = x] - P(X = x | Y = y)| < \delta$.
\end{enumerate}
For any of the PMF or CDF computation problems, a probabilistic version can be defined in which a probabilistic algorithm $A$ is said to do PMF or CDF computation iff it satisfies the above specification with probability $\geq 2/3$ on any input.

\subsection{Some hardness relationships among these formulations} \label{sec:hardness_relationships}

A number of relationships are known between the hardness of inference according to these different formulations.
Below, I will articulate a subset of these which indicate that if it is computationally hard to do 
probabilistic PMF computation up to additive error, then it is computationally difficult to solve any of the aformentinoed inference problems.
Note that the following reductions can be applied for any given observation $y$, so the results in this paper indicating that additive PMF approximation is hard on typical sets of observations imply that all of these inference problems are hard to solve on typical sets of observations.

\begin{enumerate}
    \item \textbf{Relative approximation for is at least as hard as additive approximation for equal $\epsilon$.}
    Consider a true PMF or CDF value $p$. If $\epsilon \geq 1$ then additive PMF or CDF approximation is trivial, so consider an $\epsilon < 1$.
    Since $p \in [0, 1]$, if $\hat{p}$ satisfies $\hat{p} \in (p/(1+\epsilon), (1+\epsilon)p)$, then $$|\hat{p} - p| \leq \max(|(1 + \epsilon)p - p|, |p/(1+\epsilon) - p|) \leq \max(|1 + \epsilon - 1|, |1/(1+\epsilon) - 1|) = \epsilon$$
    Thus if it is possible to compute a relative $\epsilon$ approximation to either CDF or PMF, it is also possible to compute an additive $\epsilon$ approximation to the same quantity.

    \item \textbf{CDF computation is at least as hard as PMF computation.} 
        \begin{enumerate}
            \item \textit{Exact computation.} Say $\mathsf{CDF}(x, y) := P(X \leq x | Y = y)$ can be computed exactly.  Then we can compute $\mathsf{PMF}(x, y) := P(X = x | Y = y) = \mathsf{CDF}(x, y) - \mathsf{CDF}(x - 1, y)$.
            \item \textit{Additive approximation.} Say we can compute an additive $\epsilon$ approximation $\mathsf{CDF}_\epsilon(x, y)$ to $P(X \leq x | Y = y)$. Then $\mathsf{PMF}_{2\epsilon}(x, y) := \mathsf{CDF}_\epsilon(x, y) - \mathsf{CDF}_\epsilon(x-1, y)$ is a $2\epsilon$ additive approximation to $P(X = x | Y = y)$.
            % \item \textit{Relative appoximation. }Say we can compute a relative $\epsilon$ approximation $\mathsf{CDF}'_\epsilon(x, y)$ to $P(X \leq x | Y = y)$. Then $\mathsf{PMF}_{2\epsilon}(x, y) := \mathsf{CDF}_\epsilon(x, y) - \mathsf{CDF}_\epsilon(x-1, y)$ is a $2\epsilon$ additive approximation to $P(X = x | Y = y)$.
        \end{enumerate}
    \item \textbf{Sampling is at least as hard as than probabilistic PMF computation up to additive error.} Say that it is possible to sample with pointwise error $\epsilon / 2$ from $P(X = \cdot | Y = y)$. Let $Q$ be the exact distribution which can be sampled from, which has pointwise error $\leq \epsilon / 2$ from the posterior.  Consider an algorithm which generates $N$ independent samples $x_i \sim ~$ and computes $\hat{p} := \frac{1}{N} \sum_{i=1}^N 1_{x_i = x}$.
    By the Hoeffding theorem, since $\mathbb{E}[\hat{p}] = Q(x)$, $\Pr[|\hat{p} - Q(x)| \geq \epsilon / 2] \leq 2e^{-\epsilon^2/(2N)}$. Thus if $N = 100/\epsilon^2$, $\Pr[|\hat{p} - Q(x)| \geq \epsilon / 2] << 1/3$.
    Because $|Q(x) = P(X = x | Y = y)| < \epsilon/2$, this means that
    $\Pr[|\hat{p} - P(X = x | Y = y)| > \epsilon] << 1/3$.
    Thus it is possible to compute an $\epsilon$ approximation to PMF at a cost $O(1/\epsilon^2)$ times the cost of performing approximate pointwise sampling.
    Therefore a polynomial time approximate sampling algorithm implies the existence of a polynomial time additive PMF approximation algorithm.
\end{enumerate}

\noindent See \cite{yamakami1999polynomial} or \cite{edmonds2017concepts} for a more detailed discussion of some of these relationships, and for more information about complexity theoretic formulations of computing aspects of probability distributions.

\section{Background for the main results} \label{sec:background}

\subsection{Bayesian networks and inference problems}
\begin{defn}
A \textbf{Bayesian network on binary variables} is a tuple $(V, E, P)$, where $V$ is an ordered, finite set of variables $V = \{v_1, \dots, v_n\}$, $E$ is a set of directed edges between the variables, and $P$ is a \textit{conditional probability table}.
The directed graph $(V, E)$ must be acyclic.
For $v \in V$, $\Pa(v)$ denotes the set of parent variables of $v$: $\Pa(v) = \{u : (u \mapsto v) \in E\}$.
Given any assignment $a_{\Pa(v)} \in \{0, 1\}^{|\Pa(v)|}$ to the parent variables of $v$, the conditional probability table $P$ stores value $P(v = \cdot ; a_{\Pa(v)})$, which is a probability vector $[p_{v=0}, p_{v=1}]$ in $\mathbb{R}^2$.
\end{defn}

A general Bayesian network lifts the restriction that each variable $v_i$ is binary, and allows it to have arbitrary finite domain.
In this report, I will focus on binary Bayesian networks. Because a variable with a domain of size $k$ can be represented using $\log k$ binary variables, the results proven herein carry to the general case.
Henceforth, the phrase ``Bayesian network'' should be understood as a Bayesian networtk on binary variables.

Given a Bayesian network $(P, E, V)$, we can define a joint distribution on all the variables in $V$, with probability mass function
$$
P(a) = \prod_{v_i \in V}{P(v_i = a_i ; a_{\Pa(v_i)})} \quad \forall a \in \{0, 1\}^{|V|}
$$
where $a_{\Pa(v_i)}$ is the assignment to the parent variables of $v_i$ in $a$.
I will often write $P$ to refer to the whole Bayesian network, the joint distribution on all its variables, and also marginal and conditional distributions on subsets of its variables.

A Bayesian network can be regarded as defining a probabilistic process which generates assignments to the variables in $V$.
A probabilistic computer with the ability to sample from all of the conditional distributions in the Bayesian network\footnote{
This is always possible if, for instance, the conditional distributions have probability values which are dyadic rationals, that is rationals of the form $m/2^k$ for $m, k \in \mathbb{N}$.
} can generate samples from the joint distribution $P$ by traversing the graph in topological order, and sampling a value for each variable $v$ from $P(v = \cdot | a_{\Pa(v)})$ when $v$ is encountered in the traversal.
A Bayesian network can be regarded as the probabilistic analog of a circuit on no inputs, where deterministic logic gates are replaced by conditional probability distributions.

\begin{defn}
An \textbf{inference problem} consists of a Bayesian network $(V, E, P)$, a set of \textit{observed variables} $Y \subseteq V$, a set of \textit{query variables} $X \subseteq V$, and an assignment $y \in \{0, 1\}^{|Y|}$ to the observed variables such that $P(Y = y) > 0$.
\end{defn}
The goal of an inference problem is to compute some piece of information about the posterior distribution $P(X = \cdot | Y = y)$, which is a probability distribution on $\{0, 1\}^{|X|}$.
For convenience, I will use the notation $P(X = x | Y = y)$, $P(X = x, Y = y)$, and $P(Y = y)$ to refer to conditional, joint, and marginal probability values on the assignments to the variables in variable sets $X$ and $Y$,
even though this is slightly nonstandard notation as $X$ and $Y$ here are subsets of $V$ here, and are not formally defined as random variables.

\begin{defn}
An \textbf{inference problem schema} is the tuple $I = (V, E, P, X, Y)$ as in an inference problem, but not fixing an assignment $y$ to the observed variables.
\end{defn}

\subsection{Worst and typical case inference algorithms}

\begin{defn}
A \textbf{deterministic, worst-case additive PMF approximation algorithm with tolerance $\epsilon$} is a Turing machine $A$ which on input
$(I, y, x)$, where $I$ is any inference problem schema, $y$ is any assignment to the observed variables, and $x$ is an assignment to the query variables, outputs a rational number $A(I, y, x)$ such that
\begin{equation} \label{eq:worst_case}
|A(I, y, x) - P(X = x | Y = y)| < \epsilon
\end{equation}
\end{defn}
\begin{defn}
A \textbf{probabilistic, worst-case additive PMF approximation algorithm with tolerance $\epsilon$} is a probabilistic Turing machine $A$ which for every $I, y, x$, satisfies condition~\ref{eq:worst_case} with probability $\geq 2/3$.
\end{defn}

In 1993, Dagum and Luby \cite{dagum1993} showed that if there exists a deterministic worst-case additive PMF approximation algorithm with tolerance $< 1/2$, and it has polynomial runtime, then $\P = \NP$.
Further, if a probabilistic polynomial-time worst-case additive PMF approximation algorithm exists with tolerance $< 1/2$, then $\NP \subseteq \RP$.

Bayesian networks are typically used to model aspects of the world.
That is, each variable in $V$ represents some aspect of the world; $Y$ represents a set of values which we have observed; and $X$ represents a set of values which we wish to infer.
In this setting, the probability distribution $P$ is a description of our beliefs about how probable different joint outcomes of events in the world are.
Therefore, given a Bayesian network $P$ in which we must do inference, it may be acceptable to us if there is exist assignments $y$ under which computing the posterior distribution is very expensive, so long as these instances are extremeley rare.
Since we have a probability distribution $P$ on hand which ought to roughly correspond to the distribution of $y$ values which will occur in the world, and on which we will have to run inference, a natural notion of ``rare'' is available.
Say there is a small set of observation assignments $\mathcal{Y}_\text{hard} \subseteq \{0, 1\}^{|Y|}$ such that $P(\mathcal{Y}_\text{hard}) < \rho$ for very small $\rho$ (e.g. $\rho = 0.00001$), such that for all $y \notin \mathcal{Y}_\text{hard}$, we can compute the posterior distribution $P(X = \cdot | Y = y)$ efficiently.
Then we can say that inference is easy in the typical case, and for many purposes this is sufficient.

% In fact, Dagum and Luby's construction of a worst-case inference problem $I$ involves selecting an observed assignment $y$ to a set $Y$ of one variable ($|Y| = 1$) which has extremely low marginal probability: $P(Y = y) << 1$.
% Theorem~\ref{} later in this report shows that for every inference problem schema $(P, V, E, X, Y)$ with $|Y| = 1$, efficient inference is possible on typical case observations.
% This indicates that Dagum and Luby's strategy for proving the hardness of inference in the worst case does not directly carry through to showing the hardness of inference with typical-case observations.

% \begin{defn}
% A \textbf{deterministic, typical-case relative PMF approximation algorithm with tolerances $(\epsilon, \rho)$} is a Turing machine $A$ which accepts inputs of the form
% $(I, y, x)$, where $I$ is any inference problem schema, $y$ is any assignment to the observed variables, and $x$ is an assignment to the query variables, and outputs a rational number $A(I, y, x)$ with the following property.
% For any Bayesian  network $(V, E, P)$ and any subset $Y \subseteq V$,
% there exists a ``typical set'' $\mathcal{Y}_\text{easy} \subseteq Y$ such
% that $$P(Y \in \mathcal{Y}_\text{easy}) > 1 - \rho$$ and
% for all $X \subseteq V$, any assignment $x$ to $X$,
% and any $y \in \mathcal{Y}_\text{easy}$,
% $$
% A(I, y, x) / P(X = x | Y = y) \in (\frac{1}{1 + \epsilon}, 1 + \epsilon)
% $$
% That is, on all but a small fraction $\rho$ of $y$ values, the algorithm can outputs an $\epsilon$ relative approximation to the posterior PMF value $P(X = x | Y = y)$,
% for any query variables $X$ and any query assignment $x$.
% \end{defn}

\begin{defn} \label{def:typical_case_add_alg}
A \textbf{deterministic, typical-case additive PMF approximation algorithm with tolerances $(\epsilon, \rho)$} is a Turing machine $A$ which accepts inputs of the form
$(I, y, x)$, where $I$ is any inference problem schema, $y$ is any assignment to the observed variables, and $x$ is an assignment to the query variables, and outputs a rational number $A(I, y, x)$ with the following property.
For any inference schema $I$ and any value $x$,
$$
\Pr_{y \sim P(Y = \cdot)}[|A(I, x, y) - P(X = x | Y = y)| \geq \epsilon] < \rho
$$
Given such an algorithm $A$, a given inference schema $I$, and a value $x$, I will write $\Ygood$ to denote the set
$$
\Ygood := \{y : |A(I, x, y) - P(X = x | Y = y)| < \epsilon\}
$$
Note that $P(\Ygood) > 1 - \rho$.
\end{defn}
\begin{defn}
A \textbf{probabilistic, typical-case additive PMF approximation algorithm with tolerances $(\epsilon, \rho)$} is a probabilistic Turing machine $A$ which accepts inputs $(I, y, x)$ as above, such that for any inference schema $I$ and any value $x$, there exists a set $\Ygood \subseteq \{0, 1\}^{|Y|}$ where $P(\Ygood) > 1 - \rho$ and
$$
y \in \Ygood \implies \Pr[|A(I, x, y) - P(X = x | Y = y)| < \epsilon] > 2/3
$$
where the probability is taken over the randomness of the algorithm.
\end{defn}

In this report, I prove a new result showing that if either a deterministic or probabilistic typical-case additive PMF approximation algorithm exists with tolerances $\epsilon < 1/2$, $\rho < 1/4$, then one-way functions do not exist.
This hardness result suggests that typical-case inference is computationally difficult, and it is not merely the worst-case nature of the observations considered in Dagum and Luby's reduction which made inference difficult in their setting.
Due to the typical-case formulation studied here, a different proof strategy is needed to show this hardness result.

\subsection{One-way functions and pseudorandom generators}
Proving the hardness of a computational problem $C$ is often done by reducing from an $\NP$-hard problem like $\mathsf{3SAT}$ to problem $C$, thereby showing that if $C$ could be solved in polynomial time, $\P = \NP$.
However, problems like $\mathsf{3SAT}$ are stated in terms of behavior on all, and thus worst-case, inputs.
Therefore, to prove the hardness of a computational problem on typical-case inputs, it is preferable to reduce to a hardness conjecture stated directly in terms of typical-case behavior.

In this report I will reference two such conjectures, which are known to be related.
The first conjecture is the existence of \textit{one-way functions}, functions which can be efficiently computed, but not efficiently inverted for the majority of inputs.
It is widely thought likely that such functions exist \cite{arora2009computational}, as there are a number of functions like multiplication of prime numbers, for which no inversion algorithms are known which are efficient in the typical case.
Such functions are widely used in public-key cryptography.

\begin{defn}{(Arora and Barak Def. 9.4)} \label{def:oneway}
A \textbf{one-way function} $f$ is a function $f : \{0, 1\}^* \to \{0, 1\}^*$ such that $f$ can be computed in polynomial time, and for every probabilistic polynomial time algorithm $A$, for every $c \in \mathbb{R}$,
\begin{equation} \label{eq:oneway}
n^c \cdot \Pr_{x \sim \text{Uniform}(\{0, 1\}^n)}[A(f(x), 1^n) \in f^{-1}(f(x))] \underset{n \to \infty}{\to} 0
\end{equation}
where $f^{-1}(f(x))$ is the set $\{x' \in \{0, 1\}^* : f(x') = f(x)\}$.
\end{defn}

The second conjecture is the existence of \textit{secure pseudorandom generators}, which are functions which take a short random seed and expand it into a long string which is indistinguishable from a truly random string.
This is known to follow from the existence of one-way functions, and this weaker conjecture is all the is needed for the main result of this report.
Specifically, I consider PRGs which are secure against all polynomial-time adversaries, once the strings being generated are sufficiently long.

\begin{defn}{(Weakened variant of Arora and Barak Def. 9.8)}
A \textit{secure pseudorandom generator} (PRG) is a polynomial time computable function $\{0, 1\}^* \to \{0, 1\}^*$ such that $|G(x)| = |l(|x|)|$ for some function $l : \mathbb{N} \to \mathbb{N}$, such that for every probabilistic polynomial time algorithm $A$,
\begin{equation} \label{eq:prg}
|\Pr_{s \sim \text{Uniform}(\{0, 1\}^n)}[A(G(s)) = 1] - \Pr_{y \sim \text{Uniform}(\{0, 1\}^{l(n)})}[A(y) = 1]| \underset{n \to \infty}{\to} 0
\end{equation}
The function $l$ is called the stretch of the PRG.
% I say the stretch is \textit{superlinear} if $l(n) / n \to \infty$ as $n \to \infty$.
\end{defn}
This definition is weaker than the definition given in Arora and Barak \cite{arora2009computational}, as it only requires the expression in eq.~\ref{eq:prg} to tend to zero, rather than to zero at a superpolynomial rate, as in eq.~\ref{eq:oneway}, and as in Arora and Barak's definition of a secure PRG.
The reduction in this report shows that the existence of a secure PRG, as defined here, with superlinear stretch implies the non-existence of typical-case inference algorithms with certain tolerances.
The existence of secure PRGs as defined in Arora and Barak is a strictly stronger statement than the existence of secure PRGs as defined here, and their PRGs are instances of secure PRGs as defined here.

It is known that the existence of one-way functions implies the existence of secure pseudorandom generators.
Thus, to the extent that it is believed that one-way functions exist, it is also believed that secure pseudorandom generators exist.
\begin{prop}{(Arora and Barak Thm. 9.9)} \label{prop:oneway_to_prg}
If there exists a one-way function, then there exists a secure pseudorandom generator with stretch $l(n) = n^c$.
\end{prop}
The reason I include the superpolynomial convergence rate in Def.~\ref{def:oneway} is to ensure that this theorem goes through.
As far as I am aware, it may be that given my weaker definition of a secure PRG, a weaker definition of one-way function would also suffice to imply the existence of such PRGs, but I have not verified this.

\section{Main result} \label{sec:main}
% \begin{defn}
% I call a function $l(n) : \mathbb{N} \to \mathbb{N}$ a well-behaved stretch function iff $\lim_{n \to \infty}[l(n) - n] = \infty$ and $l$ is invertible.
% \end{defn}

\begin{thm} \label{thm:main}
If there exists a polynomial-time typical-case additive PMF approximation algorithm $A$ with tolerances $\epsilon < \frac{1}{2}$, $\rho < \frac{1}{4}$,
then there does not exist a secure pseudorandom generator with an invertible stretch function $l$ such that $\lim_{n \to \infty}[l(n) - n] = \infty$.
This holds whether $A$ is deterministic or probabilistic.

\smallskip \noindent This holds even if we only require $A$ to be capable of typical-case inference in Bayesian networks with in-degree $\leq 2$ and with probability values in the set $\{0, 1/2, 1\}$.
\end{thm}

\begin{cor} \label{cor:oneway}
If there exists a polynomial-time typical-case additive PMF approximation algorithm with tolerances $\epsilon < \frac{1}{2}$, $\rho < \frac{1}{4}$,
then one-way functions do not exist.
\end{cor}
\noindent The corollary follows because if one-way functions exist, by Proposition~\ref{prop:oneway_to_prg}, there exists a secure PRG with stretch $l(n) = n^2$.

% Note that these results hold even if we only require $A$ to do inference in Bayesian networks with in-degree 2 and with probability values in the set $\{0, 1/2, 1\}$.

\smallskip
\noindent These results suggest that it is unlikely that there exists a universal typical-case inference algorithm which succeeds on a $3/4$ typical set of observations in all Bayesian networks.

\begin{proof}{(Theorem~\ref{thm:main}.)}
For contradiction, suppose $A$ is a deterministic polynomial-time typical-case additive PMF approximation algorithm with tolerances $\epsilon < \frac{1}{2}$ and $\rho < \frac{1}{4}$.
(The case where $A$ is probabilistic will be covered at the end of the proof.)
Let $\delta > 0$ be s.t. $\epsilon < \frac{1}{2} - \delta$.
Suppose $G$ is a secure PRG with invertible stretch function $l(n)$ s.t. $\lim_{n \to \infty} [l(n) - n] = \infty$.
Note that we can assume $l$ is upper-bounded by a polynomial in $n$, because if $l$ grows superpolynomially we can obtain a new PRG $G'$ which outputs the first $n^2$ bits of the output of $G$.
Under the assumption that $l$ is upper-bounded by a polynomial, note that $l^{-1}$ is computable in $\text{poly}(n)$ time, because given $m$, one can iterate through the values $n = 1, 2, \dots, m$ to find when $l(n) = m$.

\medskip
\noindent \textbf{Proof outline.}
I will construct a polynomial time Turing machine $B$ which can distinguish the distributions $\text{Uniform}(\{0, 1\}^n) \circ G^{-1}$ and $\text{Uniform}(\{0, 1\}^{l(n)})$ for all $n > N$, where $N$ is a constant.  This contradicts that $G$ is a secure PRG.

Given input $y \in \{0, 1\}^{l(n)}$,
Turing machine $B$ will guess whether it was sampled from the pseudo-random generator, or from the uniform on $\{0, 1\}^{|y|}$.
If a string $y$ is given such that $|y|$ is not in the range of $l$, then $B$ can immediately reject.
Otherwise, $B$ will compute $n = l^{-1}(|y|)$ to find the seed length needed for $G$ to produce $y$.

Second, $B$ will construct the description of an inference schema $I = (P, V, E, X, Y)$ where $|Y| = l(n)$ and $|X| = 1$, such that if $n > N$,
$$
y' \in G(\{0, 1\}^n) \implies P(X = 1 | Y = y') > 1 - \delta \quad (*)
$$
and
$$
y' \notin G(\{0, 1\}^n) \implies P(X = 1 | Y = y') = 0 \quad (**)
$$
Third, $B$ will compute $a \gets A(I, 1, y)$.
If $a < 1/2$, $B$ will output 0; otherwise $B$ will output 1.
I will first show that this implies that on typical $y$ values,
$B$ exactly decides whether $y \in \text{range}(G)$.
Specifically, there is a set $\Ygood \subseteq \{0, 1\}^{l(n)}$ with $P(\Ygood) > 3/4$ such that
$y \in \Ygood \implies B(y) = 1_{y \in \text{range}(G)}$.
Using this,
I will then show that, because we can guarantee $P(y \in \Ygood) > 3/4 = 1 - \rho$,
this implies that there exists a $\gamma > 0$ such that
$$
|\Pr_{s \sim \text{Uniform}(\{0, 1\}^n)}[B(G(s)) = 1] - \Pr_{\by \sim \text{Uniform}(\{0, 1\}^{l(n)})}[B(\by) = 1]| \geq \gamma \quad (***)
$$
for all $n > N$, contradicting that $G$ is a secure pseudorandom generator.
% This part of the proof will involve a case analysis regarding how the values in $\mathcal{Y}_\text{easy}$ are distributed among range of $G$, and its complement.

I now proceed to complete the proof by (1) proving that $B$ can construct an inference schema with properties $(*)$ and $(**)$, and (2) proving that $(***)$ holds.

\medskip
\noindent \textbf{Construction of the inference schema.}
Let $(P, V, E)$ be the Bayesian network which implements the following process.
\begin{enumerate}
    \item $P$ samples $s = s_1 s_2 \dots s_n$ uniformly at random from $\{0, 1\}^n$.
    \item $P$ samples $\bar{y} = \bar{y}_1 \bar{y}_2 \dots \bar{y}_{l(n)}$ uniformly at random from $\{0, 1\}^{l(n)}$.
    \item $P$ samples $x$ uniformly at random from $\{0, 1\}$.
    \item If $x = 0$, $P$ computes $y = \bar{y}$. Otherwise, $P$ computes $y = G(s)$.
\end{enumerate}
The set $X$ is defined to contain only $x$, and $Y$ is defined to contain the $l(n)$ bits of $y$.  This yields schema $(P, V, E, X, Y)$.

Clearly, writing a Bayesian network which does the sampling and multiplexing steps can be done in polynomial time.
(This just requires adding a bunch of binary variables to the vertex set $V$, and adding a probability table for each to $P$ specifying that each of these variables is uniformly distributed.)
Writing out the part of the Bayesian network which computes $G(s)$ can be done in polynomial time in $n$ using the Cook-Levin reduction for writing a circuit which implements the same behavior as the Turing machine for $G$, on inputs of length $n$.
Each and gate, or gate, or not gate in the circuit can be represented by a binary variable in the Bayesian network with one or two parent variables, and with a deterministic probability table that given a parent assignment, outputs the correct value for the gate.

\medskip
\noindent \textbf{Analysis of the posteriors (proof of $(*)$ and $(**)$).}
First, note that if $y \notin \text{range}(G)$,
$$
P(X = 1 | Y = y) = \frac{P(X = 1, Y = y)}{P(Y = y)} = \frac{0}{P(Y = y)} = 0
$$
because it is impossible for $P$ to sample $x = 1$ yet output a value not in the range of $G$.
This proves $(**)$.

Now, say $y \in \text{range}(G)$.
Let $m = |G^{-1}(y)|$, the number of seeds in $\{0, 1\}^n$ that get mapped to $y$.
Then
$$
P(X = 1 | Y = y) = \frac{P(X = 1, Y = y)}{P(X = 0, Y = y) + P(X = 1, Y = y)}
= \frac{
    \frac{1}{2} \frac{m}{2^n}
}{
    \frac{1}{2} \frac{1}{2^{l(n)}} + \frac{1}{2} \frac{m}{2^n}
}
$$
The numerator here is $P(X = 1, Y = y) = \frac{1}{2} \frac{m}{2^n}$ 
because $X = 1$ with probability $\frac{1}{2}$, and given that $X = 1$, we will have $Y = y$ iff the seed $s$ is in $G^{-1}(y)$, which occurs with probability $\frac{m}{2^n}$.
The denominator contains the term $P(X = 0, Y = y) = \frac{1}{2} \frac{1}{2^{l(n)}}$ because there is a $1/2$ probability that $X = 0$, and if it is $0$, $Y = y$ only if one of the $2^{l(n)}$ equally probable values in $\{0, 1\}^{l(n)}$ is chosen as $\bar{y}$.

Simplifying this, 
$$
P(X = 1 | Y = y) = \frac{
    m/2^n
}{
    1/2^{l(n)} + m/2^n
}
$$
so
$$
P(X = 0 | Y = y) = \frac{
    1/2^{l(n)}
}{
    1/2^{l(n)} + m/2^n
} = \frac{1}{1 + m 2^{l(n) - n}}
$$
Since $2^{l(n) - n} > 0$, this expression decreases in $m$.
Since $y \in \text{range}(G)$, $m \geq 1$.
Thus this expression is maximized when $m = 1$.
Therefore, for all $m$,
$$
P(X = 0 | Y = y) \leq \frac{1}{1 + 2^{l(n) - n}} < \frac{1}{2^{l(n) - n}}
$$
Let $N_1$ be an integer such that
$n > N_1 \implies l(n) - n > \log(1/\delta)$.
Then $n > N_1 \implies 2^{l(n) - n} > 1/\delta$,
so for all such $n$,
$$
P(X = 0 | Y = y) < \delta
$$
and thus
$$
P(X = 1 | Y = y) > 1 - \delta
$$
This proves $(*)$.

\medskip
\noindent \textbf{Proof that on $\Ygood$, $B$ exactly identifies $\text{range}(G)$.}
Let $\Ygood$ be the good set of observations for inference schema $I$ constructed above, and assignment $x = 1$, as defined in Definition~\ref{def:typical_case_add_alg}.
Then if the given $y$ satisfies $y \in \Ygood$, $|A(I, x, y) - P(X = x | Y = y)| < \epsilon < \frac{1}{2} - \delta$.
Thus if $y \notin \text{range}(G)$, by $(**)$, $|A(I, x, y) - 0| < \frac{1}{2} - \delta$ so $A(I, x, y) < 1/2$.
And if $y \in \text{range}(G)$ but $A(I, x, y) < 1/2$, by $(*)$ we would have $|P(X = 1 | Y = y) - A(I, x, y)| \geq |(1 - \delta) - 1/2| \geq 1/2 - \delta > \epsilon$, so it must be the case that $A(I, x, y) > 1/2$.
That is,
$$
y \in \Ygood \implies [(y \notin \text{range}(G) \implies A(I, x, y) < 1/2) \wedge (y \in \text{range}(G) \implies A(I, x, y) > 1/2)]
$$
Thus for $y \in \Ygood$, algorithm $B$ exactly decides whether $y \in \text{range}(G)$: $y \in \Ygood \implies B(y) = 1_{y \in \text{range}(G)}$.

\medskip
\noindent \textbf{Proof that $B$ identifies outputs from the PRG with nontrivial probability (proof of $(***)$).}
In this section I will write $\Pr_s$ as shorthand for $\Pr_{s \sim \text{Uniform}(\{0, 1\}^n)}$ and $\Pr_{\bar{y}}$ as shorthand for $\Pr_{\bar{y} \sim \text{Uniform}(\{0, 1\}^{l(n)})}$.

The goal of this section is to establish that there exists a $\gamma > 0$ such that
$$
|
\Pr_s[B(G(s)) = 1] - \Pr_{\bar{y}}[B(\bar{y}) = 1]
| \geq \gamma
$$
Let $p_s := \Pr_s[B(G(s)) = 1]$ and $p_\by := \Pr_{\bar{y}}[B(\bar{y}) = 1]$.
Let $p_{s, 1} := \Pr_s[B(G(s)) = 1 \wedge G(s) \in \Ygood]$,
$p_{s, 2} := \Pr_s[B(G(s)) = 1 \wedge G(s) \notin \Ygood]$,
$p_{\by, 1} = \Pr_{\bar{y}}[B(\bar{y}) = 1 \wedge \by \in \Ygood]$,
and
$p_{\by, 2} = \Pr_{\bar{y}}[B(\bar{y}) = 1 \wedge \by \notin \Ygood]$.
Observe that
$p_s = p_{s, 1} + p_{s, 2}$ and $p_\by = p_{\by, 1} + p_{\by, 2}$.
By the triangle inequality,
$$
|p_{s, 1} + p_{s, 2} - p_{\by, 1} - p_{\by, 2}| + |p_{\by, 2} - p_{s, 2}| \geq |p_{s, 1} + p_{s, 2} - p_{\by, 1} - p_{\by, 2} + p_{\by, 2} - p_{s, 2}|
$$
Canceling terms on the right hand side, we get
$$
|p_{s, 1} + p_{s, 2} - p_{\by, 1} - p_{\by, 2}| + |p_{\by, 2} - p_{s, 2}| \geq |p_{s, 1} - p_{\by, 1}|
$$
Plugging in $p_s = p_{s, 1} + p_{s, 2}$ and $p_\by = p_{\by, 1} + p_{\by, 2}$, and rearranging, we get
\begin{equation} \label{eq:triangle_inequality_application}
|p_s - p_\by| = |p_{s, 1} + p_{s, 2} - p_{\by, 1} - p_{\by, 2}| \geq |p_{s, 1} - p_{\by, 1}| - |p_{s, 2} - p_{\by, 2}|
\end{equation}

We have
\begin{multline} \label{eq:terms_ybad}
|p_{s, 2} - p_{\by, 2}| = 
\Bigl | \Pr_{s}[G(s) \notin \Ygood] \Pr_s[B(G(s)) = 1 | G(s) \notin \Ygood]
 - \Pr_\by[\by \notin \Ygood] \Pr_\by[B(\by) = 1 | \by \notin \Ygood] \Bigr |
\\
\leq \max(\Pr_s[G(s) \notin \Ygood], \Pr_\by[\by \notin \Ygood])
\end{multline}

We also have
\begin{multline} \label{eq:terms_ygood}
p_{s, 1} - p_{\by, 1} =
\Pr_s[B(G(s)) = 1 \wedge G(s) \in \Ygood] - \Pr_\by[B(\by) = 1 \wedge \by \in \Ygood]
 \\
= 
\Pr_s[G(s) \in \text{range}(G) \wedge G(s) \in \Ygood] -
\Pr_\by[\by \in \text{range}(G) \wedge \by \in \Ygood]
\\
= \Pr_s[G(s) \in \Ygood] - \Pr_\by[\by \in \text{range}(G) \wedge \by \in \Ygood] \\
\geq \Pr_s[G(s) \in \Ygood] - \Pr_\by[\by \in \text{range}(G)] 
= \Pr_s[G(s) \in \Ygood] - 2^n/2^{l(n)}
\end{multline}
The second equality here follows from the fact established above, that for $y \in \Ygood$, algorithm $B$ exactly decides whether $y \in \text{range}(G)$.

We now need to bound $\Pr_s[G(s) \in \Ygood]$ and $\Pr_\by[\by \in \Ygood]$.
Observe that
$$
P(\Ygood) = \frac{1}{2}\Pr_s[G(s) \in \Ygood] + \frac{1}{2}\Pr_\by[\by \in \Ygood]
$$
because under the model $P$, $y$ is set equal to $G(s)$ half the time and equal to $\by$ the other half of the time.
Since each of these probability terms are no greater than $1$, we have
\begin{equation} \label{eq:branch_probs_ygood}
\Pr_s[G(s) \in \Ygood] \geq 2P(\Ygood) - 1;
\quad
\Pr_\by[\by \in \Ygood] \geq 2P(\Ygood) - 1        
\end{equation}
By subtracting each side of these inequalities from 1, we also obtain
\begin{equation} \label{eq:branch_probs_ybad}
\Pr_s[G(s) \notin \Ygood] \leq 2 - 2P(\Ygood);
\quad
\Pr_\by[\by \notin \Ygood] \leq 2 - 2P(\Ygood)
\end{equation}

Combining equations \ref{eq:terms_ybad} and \ref{eq:branch_probs_ybad}, we get
\begin{equation}
|p_{s, 2} - p_{\by, 2}| \leq 2 - 2P(\Ygood)    
\end{equation}
and combining equations \ref{eq:terms_ygood} and \ref{eq:branch_probs_ygood}, we get
\begin{equation} \label{eq:term1s_err_bound}
|p_{s, 1} - p_{\by, 1}| \geq 2P(\Ygood) - 1 - 2^{n - l(n)}    
\end{equation}
Plugging in these bounds to equation~\ref{eq:triangle_inequality_application}, we obtain
\begin{equation} \label{eq:final_ps_py_bound}
|p_s - p_\by| \geq
2P(\Ygood) - 1 - 2^{n - l(n)}
- (2 - 2P(\Ygood))
= 4 P(\Ygood) - 3 - 2^{n - l(n)}
\end{equation}
Thus, 
$$
P(\Ygood) \geq \frac{3}{4} + \frac{1}{4 \cdot 2^{l(n) - n}} + \frac{1}{4}\gamma
\implies |p_s - p_\by| \geq \gamma
$$
Since $l(n) - n \to \infty$, the term $\frac{1}{4 \cdot 2^{l(n) - n}} \to 0$.
Thus since $P(\Ygood) > \frac{3}{4}$, there exists a value $\gamma > 0$ and an $N_2$ such that for all $n > N_2$,
$$
|\Pr_s[B(G(s)) = 1] - \Pr_{\bar{y}}[B(\bar{y}) = 1]| \geq \gamma
$$
Finally, it suffices to take $N = \max(N_1, N_2)$.

\medskip
\noindent
\textbf{Extending the proof to the case where the inference algorithm $A$ is probabilistic.}
This extension is conceptually straightforward; we simply need to track several additional terms in the analysis, to control the probability that algorithm $B$ fails to indicate whether $y \in \text{range}(G)$ for a value $y \in \Ygood$.
Since we can upper-bound this probability arbitrarily tightly by boosting the probabilistic version of algorithm $B$ through replication, the ultimate conclusion is the same as before.

Say that the inference algorithm $A(I, x, y)$ is probabilistic.
Write $A(I, x, y, r)$ to denote a run of algorithm $A$ that uses internal random bits $r$.
By saying $A$ is a probabilistic typical-case additive-error PMF approximation algorithm, we mean that for $y \in \Ygood$, we have
$$
\Pr_r [|A(I, x, y, r) - P(X = x | Y = y)| \geq \epsilon] < 1/3
$$
In the procedure described above, the only place where algorithm $B$ uses algorithm $A$ is to check whether $P(X = 1 | Y = y)$ is greater than $1/2$.
I will write $B(y, r)$ for a call to $B$ that uses random bitstring $r$ when calling $A$.
Our preceding analysis shows that for $y \in \Ygood$,
$$
|A(I, 1, y, r) - P(X = 1 | Y = y)| < \epsilon \implies  B(y, r) = 1_{y \in \text{range}(G)}
$$
Thus for $y \in \Ygood$,
$$
\Pr_r [B(y, r) \neq 1_{y \in \text{range}(G)}] < 1/3
$$
By calling $B$ $O(k)$ times with different random bitstrings $r$, and taking the most frequent output, we can obtain an algorithm $B^*$, consuming a longer random bitstring $r^*$, where for $y \in \Ygood$,
$$
\Pr_{r^*} [B^*(y, r^*) \neq 1_{y \in \text{range}(G)}] < 1/2^k
$$
% For convenience I will henceforth relabel $B$ and $r$ to refer to the boosted variants of the algorithm: $B := B^*$, $r := r^*$.

Except for the final section dedicated to proving statement $(***)$, the preceding proof remains essentially unchanged.
We must, however, redo part of the analysis in the final section, replacing the old deterministic algorithm $B$ with our new stochastic algorithm $B^*$.
Much of the analysis remains unchanged, except that the equations should be rewritten
so that probability expressions of the form $\Pr_s[...]$ and $\Pr_\by[...]$ which include a $B(...)$ term inside the brackets, are replaced by expressions of the form $\Pr_{s, r^*}[...]$ and $\Pr_{\by, r^*}[...]$, with the $B(...)$ terms replaced by $B^*(..., r^*)$.
% of the form $\Pr_s[B(...)]$ and $\Pr_\by[(...)]$ are replaced by expressions $\Pr_{s, r^*}[B(..., r^*)]$ and $\Pr_{\by, r^*}[B(..., r^*)]$.
% the expressions $\Pr_s$ and $\Pr_\by$ appearing in the equations should be rewritten to also close over the random bitstring $r^*$, and 
The first place where the analysis diverges nontrivially is in equation~\ref{eq:terms_ygood}.

To do the new version of the analysis, I will introduce the following notation. For $y \in \Ygood$, let $\Rgood^y$ be the set of bitstrings $r^*$ such that $B^*(y, r^*) = 1_{y \in \text{range}(G)}$,
and for $y \notin \Ygood$ let $\Rgood^y$ be the set of all bitstrings $r^*$.

Using this, in place of Equation~\ref{eq:terms_ygood}, we now have

\begin{multline} \label{eq:terms_ygood_stochastic}
    p_{s, 1} - p_{\by, 1} =
    \Pr_{s, r^*}[B^*(G(s), r^*) = 1 \wedge G(s) \in \Ygood] - \Pr_{\by, r^*}[B^*(\by, r^*) = 1 \wedge \by \in \Ygood]
     \\
    =
    \Pr_{s, r^*}[G(s) \in \text{range}(G) \wedge G(s) \in \Ygood \wedge r^* \in \Rgood^{G(s)}] + \Pr_{s, r^*}[B^*(G(s), r^*) = 1 \wedge G(s) \in \Ygood \wedge r^* \notin \Rgood^{G(s)}] \\
    - (\Pr_{\by, r^*}[\by \in \text{range}(G) \wedge \by \in \Ygood \wedge r^* \in \Rgood^\by] + \Pr_{\by, r^*}[B^*(\by, r^*) = 1 \wedge \by \in \Ygood \wedge r^* \notin \Rgood^\by]) \\
    \geq \Pr_{s, r^*}[G(s) \in \text{range}(G) \wedge G(s) \in \Ygood \wedge r^* \in \Rgood^{G(s)}] \\
    - (\Pr_{\by, r^*}[\by \in \text{range}(G) \wedge \by \in \Ygood \wedge r^* \in \Rgood^\by] + \Pr_{\by, r^*}[B^*(\by, r^*) = 1 \wedge \by \in \Ygood \wedge r^* \notin \Rgood^\by]) \\
    \geq (1 - \frac{1}{2^k}) \Pr_{s, r^*}[G(s) \in \text{range}(G) \wedge G(s) \in \Ygood] - \Pr_{\bar{y}, r^*}[\by \in \text{range}(G)] - \Pr_{\by, r^*}[r^* \notin \Rgood^\by] \\
    \geq (1 - \frac{1}{2^k}) \Pr_{s}[G(s) \in \Ygood] - 2^n/2^{l(n)} - 1/2^k
    % + \Pr_{s, r^*}[r^* \notin \Rgood^{G(s)} \wedge G(s) \in \Ygood]
    % \Pr_\by[\by \in \text{range}(G) \wedge \by \in \Ygood]
    % \\
    % = \Pr_s[G(s) \in \Ygood] - \Pr_\by[\by \in \text{range}(G) \wedge \by \in \Ygood] \\
    % \geq \Pr_s[G(s) \in \Ygood] - \Pr_\by[\by \in \text{range}(G)] 
    % = \Pr_s[G(s) \in \Ygood] - 2^n/2^{l(n)}
\end{multline}
Rewriting inequality~\ref{eq:term1s_err_bound} using this new bound yields
\begin{equation}
|p_{s, 1} - p_{\by, 1}| \geq (1 - \frac{1}{2^k})(2P(\Ygood) - 1) - 2^{n - l(n)} - 1/2^k 
\end{equation}
Equation~\ref{eq:final_ps_py_bound} becomes
\begin{multline}
|p_s - p_\by| \geq (1 - \frac{1}{2^k})(2P(\Ygood) - 1) - 2^{n - l(n)} - 1/2^k - (2 - 2P(\Ygood)) \\
= 4P(\Ygood) - 3 - 2^{n - l(n)} - \frac{2P(\Ygood)}{2^k}
\geq 4P(\Ygood) - 3 - 2^{n - l(n)} - \frac{2}{2^k}
\end{multline}
where the last inequality follow from the fact that $P(\Ygood) \leq 1$.
Therefore
\begin{equation} \label{eq:pygood_implication_stochastic}
P(\Ygood) \geq \frac{3}{4} + \frac{1}{4}(\frac{1}{2^{l(n) - n}} + \frac{2}{2^k}) + \frac{1}{4} \gamma \implies |p_s - p_\by| \geq \gamma
\end{equation}
Since we in fact have $P(\Ygood) > \frac{3}{4}$ with strict inequality,
letting $\eta := P(\Ygood) - \frac{3}{4}$, we have $\eta > 0$.
Choose $N_2, k$ large enough so that $n \geq N_2 \implies \frac{1}{2^{l(n) - n}} + \frac{2}{2^k} < \eta/2$, and set $\gamma = 2\eta$.
Because $\rho$ is a property of algorithm $A$, this choice of $k$ is independent of the length of the string $y$ input to algorithm $B^*$, so $k$ is a constant for the purposes of characterizing the complexity of algorithm $B^*$.
Thus setting $k$ to have this value does not affect the assessment that $B^*$ is a polynomial-time algorithm.
Then (\ref{eq:pygood_implication_stochastic}) holds for this $\gamma$, so there exists $\gamma > 0$ such that for all $n > N := \max(N_1, N_2)$,
$$
|\Pr_{s, r^*}[B^*(G(s), r^*) = 1] - \Pr_{\by, r^*}[B^*(\by, r^*) = 1]| \geq \gamma
$$

\end{proof}

\section{Related work}

\textbf{Marginal probability computation and counting.}
% The problem of counting the number of witnesses $y$ such that a polynomial-time Turing machine $M$ outputs $M(x, y) = 1$ on a given $x$ defines the complexity class $\#\P$, a core object of study in complexity theory.
% This is very closely related to the computation of \textit{marginal} probabilities.
% Indeed, for a Turing machine $M$ with witnesses $y$ of length $p(|x|)$,
% the number of satisfying assignments $\#M(x)$ exactly equals $2^{|x|}P(m = 1)$ in 
In this paper, I focused on the problem of posterior inference, that of computing values relating to a posterior distribution $P(X | Y = y)$.
A related problem is that of computing the marginal probability $P(Y = y)$.
This problem is widely studied in complexity theory, as for a certain class of probabilistic models, it is essentially isomorphic to counting the number of witnesses $y$ such that a polynomial-time Turing machine $M$ outputs $M(x, y) = 1$ on a given $x$.
This counting problem defines the complexity class $\#\P$, the class of functions on from $x$ to $|\{y : M(x, y) = 1\}|$ for polynomial time Turing Machines $M$. This class is a widely studied object in complexity theory.
The isomorphism in question involves considering a probability distribution on $|y| + 1$ binary values.
The first $|y|$ bits are a witness $y$ for the Turing machine, which are posited to each have an independent $\text{Bernoulli}(1/2)$ distribution.
The final bit $z$ is a flag bit, which is set to 1 if $M(x, y) = 1$ and 0 otherwise (for a given $x$).
The marginal probability $P(z = 1)$ is exactly $1/2^{|y|}$ times the number of witnesses $y$ such that $M(x, y) = 1$.
Note that while this assumes every witness $y$ to $x$ has an equal length, this assumption requires no loss of generality for time-bounded Turing machines.
For an overview of counting complexity, see Chapter 17 of Arora and Barak's textbook \cite{arora2009computational}.

\textbf{PMF computation of and sampling from marginal probability distributions.}
As $P(X = x | Y = y) := P(X = x, Y = y) / P(Y = y)$, the ability to exactly compute the marginal probability values $P(X = x, Y = y)$ and $P(Y = y)$ implies the ability to compute posterior PMF value $P(X = x | Y = y)$.
Likewise, the ability to compute each of these values up to a multiplicitive approximation error implies the ability to compute posterior PMF values up to the same error.
A related question is whether marginal probability computation enables sampling from posterior distributions.
In partial answer to this, Jerrum, Valiant, and Vazirani showed in 1986 that in important families of probabilistic models, the ability to sample approximately from a distribution is equally hard up to a polynomial factor as stochastically computing marginal probability values \cite{jerrum1986}.
In general, the ability to compute marginal probabilities is $\PP$-complete.
However, there are special cases where it has been shown that this problem is tractable.
Recently, work by Akmal and Williams and Till Tantau showed that computing the marginal probability that a k-CNF formula is satisfied, when the boolean variables are uniformly distributed, is comptuable in polynomial time \cite{akmal2022majority,tantau2022satisfaction}.

\textbf{Tractability results.}
In addition to past hardness results on the complexity of posterior inference in Bayesian networks \cite{cooper1990,dagum1993}, several tractability results are also known.
In 1997, Dagum and Luby showed that probabilistic polynomial time inference is possible in any inference schema with two restrictions: the number of observation variables is upper bounded by a constant, and the minimum probability value occurring in the Bayesian network is strictly greater than $0$ \cite{dagum1997optimal}.
For a survey of such parametrized complexity results, and several new results, see \cite{kwisthout2018approximate}.
Restrictions like those in \cite{dagum1997optimal} in effect limit the amount of information that the observation variables can carry about the query variables,
and I conjecture that in the typical-case setting considered here, these restrictions can in fact be stated explicitly as an upper bound on the Shannon mutual information between the query and observed variables.
Another setting in which some practicioners believe posterior inference is likely be tractable is in probabilistic models with bounded coupling between different latent variables \cite{freer2010probabilistic}.
It is indeed known that approximate marginal probability computation, and approximate sampling, are tractable in such settings where conditions as in the Lovasz local lemma hold \cite{moitra2019approximate}.

\textbf{Complexity of probabilistic inference in continuous models.}
Bayesian networks are broad family of probabilistic models in which to study probabilistic inference.
The main restriction of such models is that all the random variables are discrete with finite domains.
% , and there is assumed to be a sparse connectivity structure.
Discreteness is a core features of computational models like Turing machines and circuits, and in this sense Bayesian networks are a natural probabilitic analog to these models.
Many statisticians also consider probabilistic inference in classes of probabilistic models on continuous random variables.
Computations in these models can be represented in Turing machines, using algorithms which compute numerical approximations to quantities like posterior probability density function or cumulative distribution function values, and receive as input parameters controlling the requested precision for the computation.
Ackerman, Freer, and Roy have shown that under standard formulations of precision-controlled real-valued comptuation in Turing machines,
there are probabilistic models in which posterior cumulative distribution function and probability mass function computation is noncomputable \cite{ackerman2019computability}.
They also show that under many restrictions on the model family, often obeyed by statistical models on continuous random variables, inference is computable.
For a survey of models of real-valued computation using Turing machines in general, see \cite{weihrauch2012computable}.


% \section{Tractability results [very early WIP draft]}

% \textcolor{blue}{
% \textbf{Note:} I think I will probably remove this section from the version of this paper I submit for my course project.
% However, I am pretty sure I know how to prove the theorem statements here, and I think these provide a nice counterpart to the hardness result proven above.
% After the course project (or possibly before the project submission), it may be worth circling back to this section and finishing the proofs, and integrating a description of this result into the introduction and abstract.
% }

% \bigskip
% \noindent In 1997 \cite{dagum1997optimal}, Dagum and Luby followed up on their 1993 hardness result, by showing that probabilistic polynomial time inference is actually possible in any inference schema with two restrictions:
% \begin{enumerate}
%     \item The number of observation variables is bounded by a constant.
%     \item The minimum probability value occurring in the Bayesian network is strictly greater than $0$.
% \end{enumerate}
% One intuition for what these restrictions accomplish is that they prevent the observation from containing too much information about the query variables.
% In the typical-case setting, this intuition can be stated formally, and these two conditions on the Bayesian network at hand can be unified into a single condition: an upper bound on the Shannon mutual information between the query and observed variables.
% % For instance, if the minimum probability in the Bayesian network is 0.01, any given observation variable can only carry up to 

% \begin{thm} \label{thm:bounded_mutual_info}
% For any $B \in \mathbb{R}$ and any $\epsilon, \rho \in (0, 1)$,
% there exists a probabilistic polynomial time typical-case inference algorithm $A$
% with tolerances $\epsilon, \rho$, which can do inference under any inference schema such that the observation and query variables have mutual information bounded by $B$.

% \smallskip
% \noindent That is, for any
% inference schema $(P, V, E, X, Y)$ such that the mutual information $I(X; Y) < B$, and any $x \in \{0, 1\}^{|X|}$, there exists a set $\Ygood \subseteq \{0, 1\}^{|Y|}$ with $P(\Ygood) > 1 - \rho$ such that
% $$
% \Pr[|A(I, x, y) - P(X = x | Y = y)| < \epsilon] > 2/3
% $$
% where the probability is taken over the randomness of $A$.
% \end{thm}
% \begin{cor} \label{cor:small_obs_set}
% For any fixed $C$ and any $\epsilon, \rho \in (0, 1)$,
% there exists a probabilistic polynomial time typical-case inference algorithm $A$
% which can do inference in any inference schema in which the number of observation variables $|Y| < C$.
% \end{cor}
% \begin{proof}{(Corollary~\ref{cor:small_obs_set}.)}
% Say $|Y| < C$.  Then $|\{{0, 1}\}^{|Y|}| < 2^C$.
% Thus, regarding $Y$ as a random variable, the entropy $H(Y) < C$.
% Thus the mutual information $I(X ; Y) = H(Y) - H(Y | X) < C$.
% Thus a bound on the number of observed variables yieldds a bound on the mutual information between the observed and query variables, so Theorem~\ref{thm:bounded_mutual_info} can be applied.
% \end{proof}

% \begin{proof}{(Theorem~\ref{thm:bounded_mutual_info}.)}
% Say $I(X, Y) < B$.Then
% $\mathbb{E}_{x, y \sim P}[\log \frac{P(x, y)}{P(x)P(y)}] < B$.
% By lemma~\ref{lem:pqbound},
% this implies
% $$
% \Pr[\frac{P(x, y)}{P(x)P(y)} > a] \leq \frac{1}{\log a}(B + 1)
% $$
% for any $a$. Thus
% $$
% \Pr[\frac{P(x, y)}{P(x)P(y)} > 2^{T(B + 1)}] \leq \frac{1}{T(B + 1)}(B + 1) = 1/T
% $$
% Chaterjee and Diaconis's result then implies that if 
% likelihood weighting is run using $T \times 2^{T(B + 1)}$ samples,
% prodcing estimate $\hat{p}$ of $P(X = x, Y = y)$,
% then
% $$
% \mathbb{E}|\hat{p} - P(X = x, Y = y)| \leq (\frac{1}{\sqrt{T}} + 2 \sqrt{\Pr_{(x, y) \sim P}[\frac{P(x, y)}{P(x)P(y)} > 2^{T(B + 1)}]}) \leq \frac{1}{\sqrt{T}} + 2/\sqrt{T} = 3/\sqrt{T}
% $$
% Thus by Markov's inequality,
% $$
% \Pr [|\hat{p} - P(X = x, Y = y)| \geq \epsilon] \leq \frac{3}{\epsilon \sqrt{T}}
% $$
% For any $\epsilon$ and any constant $C$, we can choose $T$ large enough to make this arbitrarily small.
% Thus we can certainly ensure that the probability of $y$ where there is more than $1/3$ probability of error is less than $\rho$.

% While the constants involved here are very large, they are still constants depending only on the fixed parameters $B, \epsilon, \rho$.

% \end{proof}

% \begin{lem} \label{lem:pqbound}
% Let $P$ and $Q$ be discrete probability distributions on the same space.
% Say $\mathbb{E}_{P}[\log \frac{P}{Q}] < B$.
% Then for any $a > 1$,
% $$
% \Pr_{x \sim P}[P(x)/Q(x) > a] \leq \frac{1}{\log a}(B + 1)
% $$
% \end{lem}

\printbibliography

\end{document}
