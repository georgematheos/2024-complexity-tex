\documentclass{article}
\usepackage{amsmath}
% \usepackage{fullpage}
\usepackage{amsmath,amssymb,verbatim}
% \usepackage{fullpage}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\NP}{\mathbf{NP}}
\newcommand{\coNP}{\mathbf{coNP}}
\newcommand{\EXP}{\mathbf{EXP}}
\newcommand{\BPP}{\mathbf{BPP}}
\newcommand{\RP}{\mathbf{RP}}
\newcommand{\NEXP}{\mathbf{NEXP}}
\newcommand{\PH}{\mathbf{PH}}
\newcommand{\PSPACE}{\mathbf{PSPACE}}
\newcommand{\TIME}{\mathbf{TIME}}
\newcommand{\NTIME}{\mathbf{NTIME}}
\newcommand{\LOG}{\mathbf{LOGSPACE}}
\newcommand{\SIZE}{\mathbf{SIZE}}

\def \F {{\mathbb F}}
\def \N {{\mathbb N}}

\def \ATIME{{\mathsf{ATIME}}}
\def \NTIME{{\mathsf{NTIME}}}
\def \eps {{\varepsilon}}

\def \ASPACE{{\mathsf{ASPACE}}}
\def \SPACE{{\mathsf{SPACE}}}
\def \TIME{{\mathsf{TIME}}}
\def \BPL{{\mathbf{BPL}}}

\def \poly{\text{poly}}

\begin{document}
	
	
	\begin{center}
		\Large
		6.541/18.405 Problem Set 2
		
		\vspace{3pt}
		\normalsize
		due on {\bf Tuesday, April 2, 11:59pm}
	\end{center}
	
	{\bf Rules:} You may discuss homework problems with other students and you may work in groups, but we require that you {\em try to solve the problems by yourself before discussing them with others}. Think about all the problems on your own and do your best to solve them, before starting a collaboration. If you work in a group, include the names of the other people in the group in your written solution. {\bf Write up your {\em own} solution to every problem}; don't copy answers from another student or any other source. Cite {\bf all} references that you use in a solution (books, papers, people, websites, etc) at the end of each solution. 
	
	We encourage you to use \LaTeX, to compose your solutions. The source of this file is also available on Piazza, to get you started!
	
	{\bf How to submit:} Use Gradescope entry code \textbf{2P3PEN}.\\ \textbf{\large Please use a separate page for each problem.} 

\newpage
\section*{Problem 1: NP, BPP, RP (2 points)}

\subsection*{Question}
Prove that if $\NP \subseteq \BPP$, then $\NP = \RP$.

\emph{Hint: Try using the self-reducibility of $\mathsf{SAT}$.}

\subsection*{Answer}
Say $\NP \subseteq \BPP$.  Then $\text{SAT} \in \BPP$ so there is a ptime probabilistic Turing macine $M$ which decides SAT with probability at least 2/3.
From this we can derive a TM $\bar{M}$ which decides SAT on any formula $\phi$ with probability at least $1 - 2^{-(n + m)}$ where $n$ is the number of variables and $m$ is the number of clauses in $\phi$.

We can use $\bar{M}$ to produce an RP algorithm for $\text{SAT}$, which will try to produce a satisfying assignment $x_1, \dots, x_n$, then verify that it is correct.
First, construct formula $\phi_1$ by setting $X_1 = 1$ in $\phi$.  Run $\bar{M}$ on $\phi_1$.
If this returns that $\phi_1$ is satisfiable, set $x_1 = 1$; else set $x_1 = 0$.
Now recurse this procedure on $\phi_1$ if $x_1 = 1$, or on $\phi_{\not 1}$ generated by
fixing $X_1 = 0$ in $\phi$ if $x_1 = 0$.
Once the recursion is called on $\phi$ with no variables, if
the resulting variable-free formula reduces to false, return that there is no satisfying
assignment; else return control to the top level of the recursion.
If it ends up producing assignment $x_1, \dots, x_n$, deterministically check that this satisfies $\phi$.  If it does, return true, else, return false.

Observe that if the formula is not satisfiable, this algorithm will never return 1, since the assignment $x_1, \dots, x_n$ which is generated will not satisfy the formula.
Thus, to show that this algorithm puts $\text{SAT}$ in $\RP$, we need to show that if the formula is satisfiable, the algorithm will return 1 with probability at least 2/3.

This algorithm has $n$ recursive calls, and the probability level $i$ assigns a variable incorrectly, or incorrectly states that the formula is unsatisfiable, is at most $2^{-(i + m)}$.
Thus the overall probability something goes wrong is less than or equal to
$$
\sum_{i=1}^n{2^{-(i + m)}} < 2^{-m} < 2/3
$$

This shows that $\text{SAT} \in \RP$, and since $\RP \subseteq \NP$, we have $\NP = \RP$.


\newpage
\section*{Problem 2: A Tighter Circuit Lower Bound (2 points)}

\subsection*{Question}

We showed in class that for every $k$, there is a function $f_k \in \Sigma_3 \P$ that does not have $k n^k$-size circuits. Use the ideas in the proof of this fact to prove that there is an $\eps > 0$ and a function $f \in \EXP^{(\NP^{\NP})}$ that does not have $\eps 2^n/n$ size circuits.

\subsection*{Answer}
Algorithm $\mathsf{HardFunction}$ below implements a function f so that ``f(x) = g(x), where g is the first function such that there is no circuit of size $\epsilon 2^n/n$ that computes g''.

\begin{algorithm}{$\mathsf{HardFunction}(x : \{0, 1\}^n)$}
\begin{algorithmic}[1]
\STATE Guess $f : \{0, 1\}^n \rightarrow \{0, 1\}$ (and write it down using $2^n$ bits).
\STATE For all circuits $C$ of size $\epsilon 2^n / n$,
\STATE Guess $z$ of $n$ bits, verify $C(z) = f(z)$ and reject if not.
\STATE Then, for all $h : \{0, 1\}^n \to \{0, 1\}$, such that $h < g$ when $h$ and $g$ are represented as bit strings,
\STATE Guess a circuit $D$ of size $\epsilon 2^n / n$.
\STATE For all $z' \in \{0, 1\}^n$ verify that $D(z') = h(z')$.
\STATE Output $g(x)$.
\end{algorithmic}
\end{algorithm}

Observe that lines 3 and 6 can be implemented in Exponential(n) time.  Also observe that writing down circuits in lines 2 and 5 require $O((\epsilon 2^n / n) \log(\epsilon 2^n / n)) = O(2^n)$ bits.  Thus this algorithm can be run in exponential time with quantification taking an $\exists \forall \forall \exists$ structure (quantification on lines 1, 2, 4, and 5).  So it is in $\EXP^{\Sigma_3 \P}$.
Finally, noting that there exist functions on $n$ inputs with no circuits of size $2^n/(10n)$, we can take $\epsilon = 1/10$ to ensure that this algorithm accepts a nontrivial set of $x$ values.

Now, either $\NP \subseteq \P/\poly$ or $\NP \not\subseteq \P/\poly$.

If $\NP \subseteq \P/\poly$, then by the K-L theorem, $\PH = \Sigma_2 P$, so our function $f \in \EXP^{\Sigma_3 \P}$ actually satisfies $f \in \EXP^{\Sigma_2 P} = \EXP^{(\NP^{\NP})}$.

If $\NP \not\subseteq \P/\poly$, observe that $\text{SAT} \notin \P/\poly$.
This means that for every $k$, infinitely often in $m$, there is a string $\phi \in \{0, 1\}^m$ such that no circuit of size $km^k$ decides $\phi \in \text{SAT}$.  Thus for any $n$, there exists $m \geq 2^n/n$ which can't be solved by any circuit of size $km^k \geq k(2^n/n)^k$ and thus certainly can't be solved by a circuit of size $\epsilon 2^n/n$.  This shows that 


% (each of which can be written using $\epsilon 2^n / n \log(\epsilon 2^n / n) = O(2^n)$ bits)

\textbf{TODO}

\newpage
\section*{Problem 3: P=NP Implies Better Circuit Lower Bounds (3 Points)}

\subsection*{Question}
Prove that if $\P = \NP$ then there is an $\varepsilon > 0$ and $f \in \TIME[2^{O(n)}]$ such that $f \notin \SIZE[\varepsilon 2^n/n]$. 

\subsection*{Answer}

In the previous part, we proved that there exists an $\epsilon$ and a function $f \in \EXP^{(\NP^{\NP})}$ that does not have $\epsilon 2^n/n$ size circuits.

If $\P = \NP$, then $\EXP^{(\NP^{\NP})} = \EXP^{\NP^\P} = \EXP^{\NP} = \EXP^\P = \EXP$.  Thus there exists $f \in \EXP$ (specifically, say, $f \in \textsf{TIME}[2^{kn^k}]$) such that $f$ does not have $\epsilon 2^n/n$ size circuits.

\textbf{TODO}


\newpage
\section*{Problem 4: BPL (3 points)}

\subsection*{Question}
A probabilistic TM is said to work in space $s(n)$ if every branch requires $O(s(n))$ space for inputs of size $n$ and terminates in $2^{O(s(n))}$ time. The machine has \emph{one-way access} to a read-only random tape, i.e. each random bit can only be read once.

Define the class $\BPL$ (for bounded-error probabilistic log-space) as follows: a language $L$ is in $\BPL$ if there exists an $O(\log n)$-space probabilistic TM $M$ such that 
\begin{enumerate}
    \item If $x\in L$, then $\Pr[M(x)\text{ accepts}]\ge 2/3$.
    \item If $x\not\in L$, then $\Pr[M(x)\text{ accepts}]\le 1/3$.
\end{enumerate}
Prove the following:
\begin{enumerate}
    \item[(a)] $\BPL\subseteq \SPACE[(\log n)^2]$.
    \item[(b)] $\BPL\subseteq \P$.
\end{enumerate}

\subsection*{Answer to (a)}

Say M is a log-space probabilistic TM.

Then it uses $c log(n)$ space, and runs in $k^{k log(n)} = n^k$
time.

Given x of size n:

For every string y of length $n^k$, run M on (x, y).
Count the number of acceptances.

Return true if most of the runs accepted.

We need to track:

1. The current string y (k log(n) bits)

2. The number of acceptances (k log(n) bits)

3. Space for the current run of M (c log(n) bits).

Altogether this is O(log(n)) bits.

If this is true, then $\BPL \subseteq \textsf{SPACE}[log(n)] \subseteq \P$, solving both (a) and (b).

\textbf{Is this wrong somehow?}








\subsection*{Answer to (b)}



\newpage
\section*{Problem 5: Advice Removal (2 points)}

\subsection*{Question}
Assume $s : \N \rightarrow \N$ is a time constructible function. Recall $\mathsf{P}/s(n)$ is the class of languages decidable by a polynomial time algorithm with $s(n)$ advice. 

Prove that if $\textsf{SAT} \in \mathsf{P}/s(n)$ then $\textsf{SAT}$ can be solved in $2^{O(s(n))}\cdot \poly(n)$ time. That is, there is an algorithm running in $2^{O(s(n))}\cdot \poly(n)$ time which, given any formula $\phi$ of size $n$, outputs a satisfying assignment to $\phi$ when one exists.

(This result is interesting, in part because it is a major open problem whether $\textsf{SAT} \in \mathsf{P}/\mathsf{poly}$ implies $\P = \NP$ or not!)

\emph{Hint: Try using the self-reducibility of $\mathsf{SAT}$.}

\subsection*{Answer}

Say $\textsf{SAT} \in \P/s(n)$, witnessed by an algorithm $A(\phi, y)$ which decides whether $\phi$ is satisfiable using advice $y$, and which runs in $\poly(n)$ time.

Here is a $2^{O(s(n))} \poly(n)$ time algorithm for deciding if a given formula $\phi$ of description length $n$ is satisfiable.

For each of the $2^{s(n)}$ bit strings $y$ of length $s(n)$, run $A(\phi_1, y)$, where $\phi_1$ is $\phi$ substituted with $X_1 = 1$.
If for any $y$, $A$ says that $\phi_1$ is satisfiable, set $x_1 = 1$; else set $x_1 = 0$.

Now for each $y$, check $A(\phi_2, y)$ where $\phi_2$ is $\phi$, with $X_1 = x_1$ and $X_2 = 1$ substituted.  Continue this process through $\phi_n$ to get an assignment $x_1, x_2, \dots, x_n$.

Now, check if $\phi(x_1, x_2, \dots, x_n) = 1$, and accept if so, and reject if not.

This process certainly decides $\phi \in \mathsf{SAT}$.
And each of the $n$ steps require $2^{O(s(n))} \poly(n)$ time, so the whole algorithm uses $2^{O(s(n))} \poly(n)$ time.

\newpage
\section*{Problem 6: Pairwise Independence (6 Points, 2 for each sub-problem)}  

\subsection*{Question}
In this problem, we will show how to generate ``pseudo-random bits'' with a nice property. These are sometimes useful for derandomizing algorithms. We will use them to prove other theorems in the class.

A set of discrete random variables $X_1, X_2, X_3, \ldots, X_n$ is {\em pairwise independent} if for every $i \neq j$,
\[\Pr[ X_i = a \mid X_j = b ] = \Pr[ X_i = a]\] for every $a,b$ in the range of the variables.
$X_i$ is {\em unbiased} if each element of the range of $X_i$ is equally likely.

\begin{enumerate}

\item[(a)] {\bf Generating Pairwise Independent Bits.} Let $V_n$ be the set of all $n$-bit vectors excluding the all-$0$ vector. For every $v \in V_n$,
define $X_v (r) = \langle v, r \rangle$ where $r$ is any $n$-bit vector and $\langle,\rangle$ is inner product modulo $2$. Picking $r$ uniformly at random, the $X_v$'s are well-defined random variables. \\
{\bf Show that the $X_v$'s are unbiased and pairwise independent.} 

Note we use $n$ random bits to select $r$, and obtain $2^n - 1$ pairwise independent bits $X_v$! (One can think of the above process as a ``pairwise independent" pseudorandom generator that takes an $n$-bit seed and produces $2^n-1$ bits.)
Also note if we include the all-$0$ vector in $V_n$, we would lose the unbiased property, but the $X_i$'s would still be pairwise independent. \end{enumerate} 
\begin{enumerate}
\item[(b)] {\bf Pairwise Independent Hashing.} Let $p$ be prime and $\F_p$ be the field of integers modulo $p$. For $a, b$ independently
and uniformly chosen at random from $\F_p$, let $Y_i = a\cdot i+b $.\\
{\bf  Show that $\{Y_i \mid i\in \F_p \}$ are unbiased and pairwise independent.}

{\em Hint: For any fixed values $Y_i$ and $Y_j$, the equations $Y_i = ai+b $ and $Y_j = aj+b $ can be uniquely solved for $a$ and $b$.}  \end{enumerate} 
\begin{enumerate}
\item[(c)] We will work over $\F_2$. Let $V_n$ be the set of all $n$-bit vectors excluding the all-$0$ vector. 
Letting $M$ be a uniformly randomly chosen Boolean $m \times n$ matrix, define the random variable $X_v = Mv$. \\
{\bf Show that the $X_v$'s are unbiased and pairwise independent.}

Define the function family $f_M (v) = Mv$, as we vary over all matrices $M$. \\
{\bf Show for all $v \neq v'$, when we pick a function $f$ uniformly at random from this
family, $\Pr_f[ f(v)=a \wedge f(v')=b]$ is the same for all $a$ and $b$.}

Such a family of functions is called a {\em pairwise independent family of hash
functions}. They are very useful! 
\end{enumerate} 

\subsection*{Answer to (a)}

\textbf{Verifying unbiasedness.}
Fix $v \in V_n$.
Let $I := \{i \in \mathbb{Z}_n : v_i = 1\}$.
Observe $$X_v(r) = \sum_{i \in I} r_i \mod 2$$.
I proceed by induction on $|I|$.

If $|I| = 1$, let $i$ be the unique member of $I$.
Then $X_v(r) = r_i$, which is an unbiased random bit.

Now say we know that for any set $J$ so that $|J| = m$, $\sum_{j \in J} r_j \mod 2$ is unbiased.
Consider a set $I$ with $|I| = m + 1$.
Set $i^* := \sup I$ and set $J = I \setminus \{i^*\}$.
Then $Y(r) := \sum_{j \in J} r_j \mod 2$ is an unbiased bit.
Observe $$X_v(r) = \sum_{i \in I} r_i \mod 2 = r_{i^*} + Y(r) \mod 2$$
Here, $r_{i^*}$ and $Y(r)$ are independent unbiased bits.
It is straightforward to verify from this that $X_v(r)$ is itself an unbiased random bit, by enumerating the probability table of the 4 joint assignments to $X_v(r)$ and $Y(r)$.

\textbf{Verifying pairwise independence.}
Let $v^1, v^2 \in V_n$, with $v^1 \neq v^2$.
Let $I^1 = := \{i \in \mathbb{Z}_n : v^1_i = 1\}$ and let
$I^2 = := \{i \in \mathbb{Z}_n : v^2_i = 1\}$.

% Observe 
% $$
% X_{v^1}(r) = X_{v^2}(r) - \sum_{i \in I^2 \setminus I^1}{r_i} + \sum_{i \in I^1 \setminus I^2} r_i \mod 2
% $$
% The inductive argument used to show unbiasedness suffices to show that 

Let $I^* := I^1 \Delta I^2$ ($= I^1 \cap I^2 \setminus (I^1 \cup I^2)$).
I proceed by induction on $|I^*|$.

If $|I^*| = 1$, WLOG say $v^1_i = 1$ but $v^2_i = 0$ (otherwise swap $v^1$ and $v^2$).
Then $X_{v^1}(r) = X_{v^2}(r) + r_i \mod 2$, with $r_i$ independent from $X_{v^2}(r)$ (which is a sum of $r_j$ for $j \neq i$).
Then $X_{v^1}(r)$ is uniform on $\{0, 1\}$, conditional either on $X_{v^2}(r) = 1$ or $X_{v^2}(r) = 0$,
so $X_{v^1}(r)$ and $X_{v^2}(r)$ are independent.

Now say we know that if $|I^*| = m$, $X_{v^1}(r)$ and $X_{v^2}(r)$ are independent.
Say we have $|I^*| = m + 1$.
WLOG say $i$ is some index so $v^1_i = 1$ but $v^2_i = 0$ (if this doesn't exist, swap $v^1$ and $v^2$).
Let $\bar{v}$ be $v^1$ but with index $i$ set to 0.
Then $$\bar{x}(r) := \langle r, \bar{v} \rangle \mod 2 = X_{v^1}(r) - r_i \mod 2$$
By the inductive hypothesis, $\bar{x}(r)$ is independent from $X_{v^2}(r)$.
And it is also certainly independent from $r_i$ since it is a sum of $r_j$ all with $j \neq i$.
Thus $X_{v^1}(r) = \bar{x}(r) + r_i \mod 2$ is a function of two values independent of $X_{v^2}(r)$, and hence is independent of $X_{v^2}(r)$.
% $P[X_{v^1}(r) = 1 | X_{v}]

\subsection*{Answer to (b)}

\textbf{Unbiasedness.}
Fix $i$ and $a$.  Observe that since $b$ is uniform over $\mathbb{Z}_p$, $ai + b$ is also uniform over $\mathbb{Z}_p$.  Thus $\mathbb{P}[Y_i = y | a] = 1/p$ for all $y$.  Thus $\mathbb{P}[Y_i = 1] = \sum_{a = 0}^{p-1}\frac{1}{p}\mathbb{P}[Y_i = y | a] = \frac{1}{p}$.

\textbf{Pairwise independence.}
Fix $i$ and $j$.
We want to show that $\mathbb{P}[Y_i = y_i | Y_j = y_j] = \frac{1}{p}$.
Because $\mathbb{Z}_p$ is a field, for any value $a$ and any value $y_j$, there is exactly one value $b$ such that $y_j = a \cdot j + b$.
Thus, the event $E_j := \{Y_j = y_j\}$ equals the event $\{(a, b) : a \in \mathbb{Z}_p \wedge b = y_j - a \cdot j\}$, an event with probability $p/p^2 = 1/p$.
Since $\mathbb{Z}_p$ is a field, the system of linearly independent equations
$$
y_i = a \cdot i + b; \qquad y_j = a \cdot j + b
$$
has a unique solution $(a^*, b^*)$.
Thus $\{Y_j = y_j \wedge Y_i = y_i\} = \{a^*, b^*\}$ which has probability $1/p^2$.
Thus $\mathbb{P}[Y_i = y_i | Y_j = y_j] = \frac{\mathbb{P}[Y_i = y_i, Y_j = y_j]}{\mathbb{P}[Y_j = y_j]} = \frac{1/p^2}{1/p} = 1/p$.
\subsection*{Answer to (c)}

\textbf{Unbiasedness.} From part (a) we know that $(Mv)_i$ is a uniform bit for each $i$, where $(Mv)_i$ is the $i$th element of vector $Mv$.  Furthermore, since $(Mv)_i$ depends on a different row of $M$ for each $i$, the collection of values $\{(Mv)_i\}_i$ is an independent collection of random variables.  Thus $Mv$ is uniformly distributed on the space of binary n-dimensional matrices.

\textbf{Pairwise independence.}
Fix $v^1, v^2$.
Observe that 

\begin{multline*}
\mathbb{P}[Mv^1 = v | Mv^2] = \\
\mathbb{P}[(Mv^1)_0 = v_0 | Mv^2] \times \\
\mathbb{P}[(Mv^1)_1 = v_1 | Mv^2, (Mv^1)_0] \times \dots \times \\
\mathbb{P}[(Mv^1)_{n-1} = v_{n-1} | Mv^2, (Mv^1)_{0:n-2}]
\end{multline*}

For any $i$,
$$
\mathbb{P}[(Mv^1)_i = v_i | Mv^2, (Mv^1)_{0:i-1}] =
\mathbb{P}[(Mv^1)_i = v_i | Mv^2_i]
$$
because all the other values being conditioned on depend on entirely independent rows of $M$ from those that affect the value $(Mv^1)_i$.
And from part (a), we know $(Mv^1)_i$ and $(Mv^2)_i$ are independent for all $i$, so $\mathbb{P}[(Mv^1)_i = v_i | Mv^2_i] = 1/2$.
Thus $\mathbb{P}[Mv^1 = v | Mv^2] = 1/2^n = \mathbb{P}[Mv^1 = v]$.

\textbf{Uniformity on joint assignments.}
$$
\mathbb{P}[f(v) = a \wedge f(v') = b] =
\mathbb{P}[f(v) = a]\times\mathbb{P}[f(v') = b | f(v) = a]
= \frac{1}{2^n} \times \frac{1}{2^n}
$$
where the last equality follows due to the pairwise independence and uniformity proven above.

\end{document}