\documentclass{article}
\usepackage{amsmath}
% \usepackage{fullpage}
\usepackage{amsmath,amssymb,verbatim}
% \usepackage{fullpage}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\renewcommand{\P}{\mathbf{P}}
\newcommand{\NP}{\mathbf{NP}}
\newcommand{\coNP}{\mathbf{coNP}}
\newcommand{\EXP}{\mathbf{EXP}}
\newcommand{\BPP}{\mathbf{BPP}}
\newcommand{\RP}{\mathbf{RP}}
\newcommand{\NEXP}{\mathbf{NEXP}}
\newcommand{\PH}{\mathbf{PH}}
\newcommand{\PSPACE}{\mathbf{PSPACE}}
\newcommand{\TIME}{\mathbf{TIME}}
\newcommand{\NTIME}{\mathbf{NTIME}}
\newcommand{\LOG}{\mathbf{LOGSPACE}}
\newcommand{\SIZE}{\mathbf{SIZE}}

\def \F {{\mathbb F}}
\def \N {{\mathbb N}}

\def \ATIME{{\mathsf{ATIME}}}
\def \NTIME{{\mathsf{NTIME}}}
\def \eps {{\varepsilon}}

\def \ASPACE{{\mathsf{ASPACE}}}
\def \SPACE{{\mathsf{SPACE}}}
\def \TIME{{\mathsf{TIME}}}
\def \BPL{{\mathbf{BPL}}}

\def \poly{\text{poly}}

\begin{document}
	
	
	\begin{center}
		\Large
		6.541/18.405 Problem Set 2
		
		\vspace{3pt}
		\normalsize
		due on {\bf Tuesday, April 2, 11:59pm}
	\end{center}
	
	{\bf Rules:} You may discuss homework problems with other students and you may work in groups, but we require that you {\em try to solve the problems by yourself before discussing them with others}. Think about all the problems on your own and do your best to solve them, before starting a collaboration. If you work in a group, include the names of the other people in the group in your written solution. {\bf Write up your {\em own} solution to every problem}; don't copy answers from another student or any other source. Cite {\bf all} references that you use in a solution (books, papers, people, websites, etc) at the end of each solution. 
	
	We encourage you to use \LaTeX, to compose your solutions. The source of this file is also available on Piazza, to get you started!
	
	{\bf How to submit:} Use Gradescope entry code \textbf{2P3PEN}.\\ \textbf{\large Please use a separate page for each problem.} 

\newpage
\section*{Problem 1: NP, BPP, RP (2 points)}

\subsection*{Question}
Prove that if $\NP \subseteq \BPP$, then $\NP = \RP$.

\emph{Hint: Try using the self-reducibility of $\mathsf{SAT}$.}

\subsection*{Answer}
Say $\NP \subseteq \BPP$.  Then $\text{SAT} \in \BPP$ so there is a polynomial time probabilistic Turing machine $M$ which decides SAT with probability at least 2/3.
From this we can derive a TM $\bar{M}$ which decides SAT on any formula $\phi$ with probability at least $1 - 2^{-(n + m)}$ where $n$ is the number of variables and $m$ is the number of clauses in $\phi$.

We can use $\bar{M}$ to produce an RP algorithm for $\text{SAT}$, which will try to produce a satisfying assignment $x_1, \dots, x_n$, then verify that it is correct.
First, construct formula $\phi_1$ by setting $X_1 = 1$ in $\phi$.  Run $\bar{M}$ on $\phi_1$.
If this returns that $\phi_1$ is satisfiable, set $x_1 = 1$; else set $x_1 = 0$.
Now recurse this procedure on $\phi_1$ if $x_1 = 1$, or on $\phi_{\neg 1}$ generated by
fixing $X_1 = 0$ in $\phi$ if $x_1 = 0$.
Now let $\phi$ be replaced with the restriction of $\phi$ to $X_1 = x_1$.
Shift down the index of each variable (so what was called $X_2$ is now called $X_1$, etc.).
Now recurse this procedure.
This will ultimately produce a formula $\phi$ with no variables.
If the resulting variable-free formula reduces to false, return that there is no satisfying
assignment.
If the formula reduces to true, consider the assignment $x_1, \dots, x_n$ produced by this process.
Deterministically check that it satisfies $\phi$.
If it does, return true, else, return false.

Observe that if the formula is not satisfiable, this algorithm will never return 1, since the assignment $x_1, \dots, x_n$ which is generated will not satisfy the formula.
Thus, to show that this algorithm puts $\text{SAT}$ in $\RP$, we need to show that if the formula is satisfiable, the algorithm will return 1 with probability at least 2/3.

This algorithm has $n$ recursive calls, and the probability level $i$ assigns a variable incorrectly, or incorrectly states that the formula is unsatisfiable, is at most $2^{-(i + m)}$.
Thus the overall probability something goes wrong is less than or equal to
$$
\sum_{i=1}^n{2^{-(i + m)}} < 2^{-m} < 2/3
$$

\noindent This shows that $\text{SAT} \in \RP$, and since $\RP \subseteq \NP$, we have $\NP = \RP$.


\newpage
\section*{Problem 2: A Tighter Circuit Lower Bound (2 points)}

\subsection*{Question}

We showed in class that for every $k$, there is a function $f_k \in \Sigma_3 \P$ that does not have $k n^k$-size circuits. Use the ideas in the proof of this fact to prove that there is an $\eps > 0$ and a function $f \in \EXP^{(\NP^{\NP})}$ that does not have $\eps 2^n/n$ size circuits.

\subsection*{Answer}
% Algorithm $\mathsf{HardFunction}$ below implements a function f so that ``f(x) = g(x), where g is the first function such that there is no circuit of size $\epsilon 2^n/n$ that computes g''.

% \begin{algorithm}{$\mathsf{HardFunction}(x : \{0, 1\}^n)$}
% \begin{algorithmic}[1]
% \STATE Guess $f : \{0, 1\}^n \rightarrow \{0, 1\}$ (and write it down using $2^n$ bits).
% \STATE For all circuits $C$ of size $\epsilon 2^n / n$,
% \STATE Guess $z$ of $n$ bits, check $C(z) = f(z)$ and reject if so.
% \STATE Then, for all $h : \{0, 1\}^n \to \{0, 1\}$, such that $h < g$ when $h$ and $g$ are represented as bit strings,
% \STATE Guess a circuit $D$ of size $\epsilon 2^n / n$.
% \STATE For all $z' \in \{0, 1\}^n$ verify that $D(z') = h(z')$.
% \STATE Output $g(x)$.
% \end{algorithmic}
% \end{algorithm}

% Observe that lines 3 and 6 can be implemented in Exponential(n) time.  Also observe that writing down circuits in lines 2 and 5 require $O((\epsilon 2^n / n) \log(\epsilon 2^n / n)) = O(2^n)$ bits.  Thus this algorithm can be run in exponential time with quantification taking an $\exists \forall \forall \exists$ structure (quantification on lines 1, 2, 4, and 5).  So it is in $\EXP^{\Sigma_3 \P}$.
% Finally, noting that there exist functions on $n$ inputs with no circuits of size $2^n/(10n)$, we can take $\epsilon = 1/10$ to ensure that this algorithm accepts a nontrivial set of $x$ values.

% Now, either $\NP \subseteq \P/\poly$ or $\NP \not\subseteq \P/\poly$.

% If $\NP \subseteq \P/\poly$, then by the K-L theorem, $\PH = \Sigma_2 P$, so our function $f \in \EXP^{\Sigma_3 \P}$ actually satisfies $f \in \EXP^{\Sigma_2 P} = \EXP^{(\NP^{\NP})}$.

% If $\NP \not\subseteq \P/\poly$, observe that $\text{SAT} \notin \P/\poly$.
% This means that for every $k$, infinitely often in $m$, there is a string $\phi \in \{0, 1\}^m$ such that no circuit of size $km^k$ decides $\phi \in \text{SAT}$.  Thus for any $n$, there exists $m \geq 2^n/n$ which can't be solved by any circuit of size $km^k \geq k(2^n/n)^k$ and thus certainly can't be solved by a circuit of size $\epsilon 2^n/n$.

% Guess a SAT problem of size $2^n/n$.  Verify that for all $k$ 


% % (each of which can be written using $\epsilon 2^n / n \log(\epsilon 2^n / n) = O(2^n)$ bits)

% \textbf{TODO}

% \newpage

\begin{algorithm}{$\mathsf{HardFunction}(x : \{0, 1\}^n)$}
\begin{algorithmic}[1]
	\STATE \COMMENT{
		This is the lexographically first hard function from $\{0, 1\}^n$ to $\{0, 1\}$.
	}
	\STATE Initialize a list of bits $L$ with $2^n$ slots.  Fill them all with 0.
	\STATE \COMMENT{$L$ will store a truth table for the hard function.}
	\FOR{$j = 0, 1, \dots, 2^{n} - 1$}
	\STATE \COMMENT{This loop writes a truth table for the hard function to list $L$.}
	\STATE $b \gets \mathsf{CheckIfHardCompletionExists}(L, j, 1^{2^n})$
	\STATE $L[j] \gets \neg b$
	\ENDFOR
	\RETURN $L[x]$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}{$\mathsf{CheckIfHardCompletionExists}(L, j, 1^n)$}
	\begin{algorithmic}[1]
		\STATE \COMMENT{
			Given a list $L$ with $x - 1$ bits, return 1 iff there exists a function
			$g : \{0, 1\}^n \to \{0, 1\}$ such that $g(i) = L(i)$ for all $i < x$, and where $g(x) = 0$.
		}
		\STATE Existentially guess a sequence $B$ of $2^n - x$ bits.
		\STATE Universally choose a circuit $C$ of size $\epsilon 2^n/n$.
		\FOR{$z \in \{0, 1\}^n$}
			\STATE \COMMENT{Compute $b \gets g(z)$.}
			\IF{$z = x$}
				\STATE $b \gets 0$
			\ELSIF{$z < x$}
				\STATE $b \gets L[z]$
			\ELSE
				\STATE $b \gets B[z - x]$
			\ENDIF

			\STATE \COMMENT{Verify $\exists z . C(z) \neq g(z)$.}
			\IF{$C(z) \neq b$}
				\RETURN 1
			\ENDIF
		\ENDFOR
		\RETURN 0
	\end{algorithmic}
\end{algorithm}


Algorithm $\mathsf{HardFunction}$ implements the lexographically first function $g$ from $\{0, 1\}^n$ to $\{0, 1\}$ such that no circuit of size $\epsilon 2^n/n$ computes $g$.
Such a function exists if we take $\epsilon = 1/10$, for instance.

$\mathsf{HardFunction}$ works by explicitly constructing a truth table for $g$, by writing down a truth table $L$ of $2^n$ bits.  It then returns $L[x]$.
To construct the lexographically first string, when writing position $j$ into the truth table, $\mathsf{HardFunction}$ checks if it is possible to turn the current list $L$ into a full hard function if $L[j]$ is set to 0.  If this is possible, it does this; otherwise it resorts to setting $L[j] \gets 1$.

The procedure requires running $2^n$ checks, each of which requires writing $1^{2^n}$ onto the tape to call $\mathsf{CheckIfHardCompletionExists}$.
Therefore the algorithm takes $O(2^n \times 2^n) = O(2^{2n}) = 2^{O(n)}$ time, if we have an oracle for $\mathsf{CheckIfHardCompletionExists}$.

Finally, we argue that indeed given an $\NP^\NP = \Sigma_2^P$ oracle, we can implement procedure $\mathsf{CheckIfHardCompletionExists}$.
The procedure existentially guesses a function $g$ consistent with the partial truth table $L$ and the choice $g(j) = 0$, and then verifies that for all circuits $C$, $C$ does not implement $g$.  (This last check requires looping over $2^n$ values $z \in \{0, 1\}^n$.)  This is in $\Sigma_2^P$ with respect to the input which has size $\geq 2^n$.

\textbf{Acknowledgements.} I asked ChatGPT if $\EXP^{\Sigma_2^P}$ and $\Sigma_2^{\EXP}$ were the same (and it said no), which helped me discover the thread of thought I needed to follow for this problem.
% Initialize a list L.  L[j] will store the value of a function g at value j.
% For x = 0, 1, 2, ..., 2^n-1:
%   (*) Check if there is a function g so that g(x) = 1 and g(j) = L[j]
%   for each j < x, and such that g isn't solved by $\epsilon 2^n/n$ circuits.
%   If not, reject x.  (We are saying f(x) = 0.)
% Return L[input].

% Procedure (*):
%   Input: x in {0, 1, .., 2^n-1}, list L
%   Output: does there exist a function g consistent with L where g(x) = 1 and no circuits exist
%   Procedure:
%     Guess a completion g of list L where g(x) = 1.
% 	For all circuits C of size $\epsilon 2^n/n$:
% 	Verify that there exists an n-bit z such that C(z) != g(z).
% 	  If such a z exists, return 1; else return 0.











\newpage

\section*{Problem 3: P=NP Implies Better Circuit Lower Bounds (3 Points)}

\subsection*{Question}
Prove that if $\P = \NP$ then there is an $\varepsilon > 0$ and $f \in \TIME[2^{O(n)}]$ such that $f \notin \SIZE[\varepsilon 2^n/n]$. 

\subsection*{Answer}

% In the previous part, we proved that there exists an $\epsilon$ and a function $f \in \EXP^{(\NP^{\NP})}$ that does not have $\epsilon 2^n/n$ size circuits.

% If $\P = \NP$, then $\EXP^{(\NP^{\NP})} = \EXP^{\NP^\P} = \EXP^{\NP} = \EXP^\P = \EXP$.  Thus there exists $f \in \EXP$ (specifically, say, $f \in \textsf{TIME}[2^{kn^k}]$) such that $f$ does not have $\epsilon 2^n/n$ size circuits.


% \textbf{TODO}

% \bigskip
In the previous problem we showed that $\mathsf{HardFunction}$ can be implemented in $O(2^n(2^n + K(2^n)))$ time, where $K(m)$ is the runtime of $\mathsf{CheckIfHardCompletionExists}$.
We also showed that $\mathsf{CheckIfHardCompletionExists} \in \NP^\NP$.
Thus if $\P = \NP$, $\mathsf{CheckIfHardCompletionExists} \in \P$, so $K(m)$ is a polynomial, say $km^k$.
Then $\mathsf{HardFunction}$ can be run in $O(2^n(2^n + k(2^n)^k))$.
This simplifies to $O(2^{2n} + 2^{n + kn})$, which is $2^{O(n)}$.
Since no circuits of size $\epsilon 2^n/n$ can decide $\mathsf{HardFunction}$, this yields the desired result.


\newpage
\section*{Problem 4: BPL (3 points)}

\subsection*{Question}
A probabilistic TM is said to work in space $s(n)$ if every branch requires $O(s(n))$ space for inputs of size $n$ and terminates in $2^{O(s(n))}$ time. The machine has \emph{one-way access} to a read-only random tape, i.e. each random bit can only be read once.

Define the class $\BPL$ (for bounded-error probabilistic log-space) as follows: a language $L$ is in $\BPL$ if there exists an $O(\log n)$-space probabilistic TM $M$ such that 
\begin{enumerate}
    \item If $x\in L$, then $\Pr[M(x)\text{ accepts}]\ge 2/3$.
    \item If $x\not\in L$, then $\Pr[M(x)\text{ accepts}]\le 1/3$.
\end{enumerate}
Prove the following:
\begin{enumerate}
    \item[(a)] $\BPL\subseteq \SPACE[(\log n)^2]$.
    \item[(b)] $\BPL\subseteq \P$.
\end{enumerate}

\subsection*{Answer to (a)}

Let $L$ be in $\BPL$, witnessed by a log-space Turing Machine $T$ with a $2/3$ promise.
Given $x$ of length $n$, let $C$ be the set of confirugations for $T$ on $x$.
W.l.o.g. let $c_\text{init}$ be the unique initial state and let $c_\text{acc}$ be the unique accepting state.
Let $kn^k$ be an upper bound on the runtime of $T$.
Let $T_0, T_1 : C \to C$ be the functions describing the configuration $T$ transitions to from a given state $c \in C$, where $T_0(c)$ is the new state if a 0 is read from the random tape, and $T_1(c)$ is the new state if a 1 is read from the random tape.
Let $M$ be the $|C| \times |C|$ matrix where $M_{i, j}$ is the probability that $T$ eventually passes through state $j$, given that it began in state $i$.
Our goal is to find a $\log(n)^2$ space Turing machine which can determine whether $M_{c_\text{init}, c_\text{acc}} \geq 2/3$.

Let $M^m$ be the $|C| \times |C|$ matrix where $M^m_{i, j}$ is the probability of going from configuration $i$ to configuration $j$ in $\leq m$ steps.
Then $M = M^{kn^k}$.
Observe that any element $M^m_{i, j}$ can be computed by enumerating over all sequences of $m$ random bits and counting the number of transitions from $i$ to $j$.
Also observe that
$$
M_{i, j}^{2^m} = \sum_{c \in C} M_{i, c}^{2^{m-1}} M_{c, j}^{2^{m-1}}
$$

The algorithm $\mathsf{ApproximateTransitionProbability}(T, i, j, m, b)$ below approximately computes $M_{i, j}^{2^m}$, and represents the result
as a $b$-bit binary fraction.
(Approximation is forced by needing to store the result in $b$ bits.)

Say $\hat{M}_{i, j}^{kn^k}$ is the value returned by
$$\mathsf{ApproximateTransitionProbability}(T, c_\text{init}, c_\text{acc}, \log(kn^k), a \log(n))$$
where $a$ is a constant independent of the input $x$.
(Note that for simplicity, I will henceforth assume $kn^k$ is a power of 2, to avoid complicating the math by rounding $kn^k$ up to the next power of 2 in all the following expressions.)

Say we can show that the following two properties hold:
\begin{enumerate}
	\item $|M_{i, j}^{kn^k} - \hat{M}_{i, j}^{kn^k}| < \frac{1}{10}$
	\item $\mathsf{ApproximateTransitionProbability}(T, c_\text{init}, c_\text{acc}, \log(kn^k), a \log(n))$ \\
	uses $O((\log n)^2)$ space
\end{enumerate}
Then $L \in \SPACE[(\log n)^2]$, because we can decide $x \in L$ by computing $\hat{M}_{i, j}^{kn^k}$ and returning true iff $M_{i, j}^{kn^k} \geq \frac{2}{3} - \frac{1}{10}$.

\begin{algorithm}{$\mathsf{ApproximateTransitionProbability}(T, i, j, m, b)$}
	\begin{algorithmic}[1]
	\STATE \COMMENT{
		Approximate $\mathbb{P}[T \text{ transitions } \text{from } i \text{ to } j \text{ within } m \text{ steps}]$.
		Output the answer using $b$ bits (ie. output $0.x_1x_2\dots x_b$).
	}
	\STATE $p \gets 0$ \COMMENT{Transition probability.}
	\IF{m = 0}
	  \FOR{$v \in \{0, 1\}$}
		\IF{$T_v(i) = j$}
		  \STATE $p \gets p + \frac{1}{2}$
		\ENDIF
	  \ENDFOR
	\ELSE
	  \FOR{$c \in C$}
		\STATE $p_1^c \gets \mathsf{ApproximateTransitionProbability}(T, i, c, m-1, b)$
		\STATE $p_2^c \gets \mathsf{ApproximateTransitionProbability}(T, c, j, m-1, b)$
		\STATE $p \gets p + (p_1^c \times p_2^c)$
	  \ENDFOR
	  \STATE $\hat{p} \gets p$ rounded to $b$ bits
	\ENDIF
	\RETURN $\hat{p}$
	\end{algorithmic}
\end{algorithm}

I will now prove that properties (1) and (2) hold.

\medskip
\noindent \textbf{Proof of property 1 (error analysis).}
Observe that the final rounding step, which converts $p$ to $\hat{p}$ by rounding to $b$ bits, introduces no more than $\frac{1}{2^b}$ error, ie.
$$
|p - \hat{p}| \leq \frac{1}{2^b}
$$

Say that for all $m$, $i$, and $j$,
$$
|M_{i, j}^{2^m} - \hat{M}_{i, j}^{2^m}| < \delta_m
$$

Observe that we can take $\delta_0 = 0$ so long as $b \geq 1$.
I will now upper bound $\delta_m$ in terms of $\delta_{m-1}$.

Consider the value of $p$ computed for a particlar $m$, $i$, and $j$.  We have

\begin{multline*}
	|p - M_{i, j}^{2^m}|
	= |\sum_{c \in C}{(\hat{M}_{i, c}^{2^{m-1}} \times \hat{M}_{c, j}^{2^{m-1}})} - \sum_{c \in C}{(M_{i, c}^{2^{m-1}} \times M_{c, j}^{2^{m-1}})}| \\
	= |\sum_{c \in C}{(\hat{M}_{i, c}^{2^{m-1}} \times \hat{M}_{c, j}^{2^{m-1}}} - {M_{i, c}^{2^{m-1}} \times M_{c, j}^{2^{m-1}})}| \\
	= |
	\sum_c {M^{2^{m-1}}_{i, c}(\hat{M}^{2^{m-1}}_{c, j} - M^{2^{m-1}}_{c, j})} -
	\sum_c {\hat{M}^{2^{m-1}}_{c, j}(M_{i, c} - \hat{M}^{2^{m-1}}_{i, c})}
	| \\
	\leq |\sum_c {M^{2^{m-1}}_{i, c}(\hat{M}^{2^{m-1}}_{c, j} - M^{2^{m-1}}_{c, j})}|
	+ |\sum_c {\hat{M}^{2^{m-1}}_{c, j}(M^{2^{m-1}}_{i, c} - \hat{M}^{2^{m-1}}_{i, c})}| \\
	\leq \sum_c {M^{2^{m-1}}_{i, c} \delta_{m-1}} + \sum_c {\hat{M}^{2^{m-1}}_{c, j}\delta_{m-1}} \\
	\leq \delta_{m-1} + \sum_c {\hat{M}^{2^{m-1}}_{c, j}\delta_{m-1}} \\
	\leq \delta_{m-1} + \delta_{m-1}(1 + |C|\delta_{m-1})
	\\ = 2\delta_{m-1} + |C|\delta_{m-1}^2
\end{multline*}

The second to last inequality uses $\sum_c {M^{2^{m-1}}_{i, c}} = 1$ (which is true since $M^{2^{m-1}}_{i, c}$ is a probability distribution on $c$).
The last inequality uses the fact that $\sum_c \hat{M}^{2^{m-1}} \leq 1 + |C|\delta_{m-1}$, which uses the fact that each $\hat{M}^{2^{m-1}}$ is within $\delta_{m-1}$ of $M^{2^{m-1}}$, and these values form a probability distribution.


\newpage
\begin{multline*}
	|p - M_{i, j}^{2^m}| = |\sum_{c \in C}{(\hat{M}_{i, c}^{2^{m-1}} \times \hat{M}_{c, j}^{2^{m-1}})} - \sum_{c \in C}{(M_{i, c}^{2^{m-1}} \times M_{c, j}^{2^{m-1}})}|
	\\
	\leq \sum_{c \in C}{
		\hat{M}_{i, c}^{2^{m-1}} \times \hat{M}_{c, j}^{2^{m-1}} -
		\sum_{c \in C}{(M_{i, c}^{2^{m-1}} \times M_{c, j}^{2^{m-1}})}
	} \\
	\leq \sum_{c \in C}{(|\hat{M}_{i, c}^{2^{m-1}} - M_{i, c}^{2^{m-1}}| + |\hat{M}_{c, j}^{2^{m-1}} - M_{c, j}^{2^{m-1}}|)} \\
	\leq \sum_{c \in C} 2\delta_{m-1} = 2|C|\delta_{m-1}
\end{multline*}
The second inequality follows because the values of $M$ are no greater than 1.

Combining this bound with our bound on $|p - \hat{p}|$ above, and noting that $\hat{M}_{i, j}^{2^m} = \hat{p}$ by definition, we derive that we can have $\delta_m$ satisfy
$$
\delta_m \leq 2|C|\delta_{m-1} + \frac{1}{2^b}
$$

A simple induction proof verifies that this means we can have
$$
\delta_m \leq \frac{1}{2^b} \sum_{j=0}^{m-1}(2 |C|)^j
$$
Because $2|C| \geq 2$, this bound implies
$$
\delta_m \leq \frac{(2|C|)^m}{2^b} = 2^{
	\log(2|C|)m - b
}
$$
Observe that if
$$
b > \log(2|C|)m - \log(\epsilon)
$$
then $\delta_m \leq \epsilon$.

At the top level of the recursion, $m = \log(k n^k) = \theta(\log(n))$.
Also, $|C| \leq kn^k$.
Thus, to achieve an $\epsilon$ error bound for arbitrary $\epsilon$ it suffices to have
$$
b > \theta(\log(2kn^k)\log(n) - \log(\epsilon)) = \theta(k\log(n)^2 - \log(\epsilon)) = \theta(\log(n)^2)
$$

I have not figured out how to reduce this to $\theta(\log(n))$ rather than $\theta(\log(n)^2)$, as required to achieve the space bound this problem asks for.

\medskip
\noindent \textbf{Proof of property 2 (space complexity analysis).}
We now need to analyze the space utilization of $$\mathsf{ApproximateTransitionProbability}(T, i, j, kn^k, a \log(n))$$
This algorithm recurses $m = \theta(\log(n))$ times and at each level of the recursion it needs to store $\theta(\log(n))$ bits.
Therefore in total it uses $\theta(\log(n)^2)$ bits, as required.

% \begin{algorithm}{$\mathsf{COUNT}(T, i, j, m)$}
% \begin{algorithmic}[1]
% \STATE $n \gets 0$ \COMMENT{Number of paths $i \mapsto j$ in $2^m$ steps.}
% \IF{m = 0}
%   \FOR{$b \in \{0, 1\}$}
%     \IF{$T_b(i) = j$}
% 	  \STATE $n \gets n + 1$
% 	\ENDIF
%   \ENDFOR
% \ELSE
%   \FOR{$c \in C$}
%     \STATE $n_1 \gets \mathsf{COUNT}(T, i, c, m-1)$
% 	\STATE $n_2 \gets \mathsf{COUNT}(T, c, j, m-1)$
% 	\STATE $n \gets n + (n_1 \times n_2)$
%   \ENDFOR
% \ENDIF
% \RETURN $n$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}{$\mathsf{DecideBPL}(T, n, k, c_\text{init}, c_\text{acc})$}
% \begin{algorithmic}[1]
%   \STATE $n \gets \mathsf{COUNT}(T, c_\text{init}, c_\text{acc}, kn^k)$
%   \IF{$n \geq \frac{2}{3} 2^{2^{kn^k}}$}
% 	\RETURN 1
%   \ELSE
% 	\RETURN 0
%   \ENDIF
% \end{algorithmic}
% \end{algorithm}

% We will develop an algorithm which constructs a PRG $g : \{0, 1\}^{\log n} \to \{0, 1\}^n$ which lets us approximate each $M^m$.
% Let $M^{m, g}_{i, j}$ denote the probability that $T$ transitions from $i$ to $j$ in $m$ steps, given that its random bit tape is populated by $g(s)$ for a uniformly random seed $s \in \text{domain}(g)$.

% We will do this gradually by building up a sequence $g_0, g_1, g_2, \dots, g_{\log n}$, where $g_i : \{0, 1\}^i \to \{0, 1\}^{2^i}$ such that
% $$
% || M^{2^i, g_i} - M^{2^i}||_{\infty} < \delta_i
% $$
% (I will decide what the $\delta_i$ need to be later.)  $|| \cdot ||_\infty$ denotes the supremum norm.

% We will let $g_i$ be the pairwise independent 



\medskip
\noindent \textbf{Acknowledgements.}  I read parts of Noam Nisan 1992, ``$\text{RL} \subseteq \text{SC}$'' while working on this problem.
I also want to thank Zixuan for helpful answers to a couple of questions on Piazza.

\newpage
\subsection*{Answer to (b)}

The above procedure can be used to decide $L$ in polynomial time.  Thus $\BPL \subseteq \P$.

\medskip
\noindent \textbf{Runtime analysis.}
Consider the runtime of this procedure.
Let $m^* = \log(kn^k)$.
The procedure produces a tree of calls with 1 call with value $m = m^*$, 2 calls with value $m = m^*-1$, 4 with value $m = m^*-2$, and so on up to $2^{m^*}$ calls with value 0.
This in total is $\leq 2^{m^*+1}$ calls.
Each recursive call requires multiplying two $b$-bit values and summing $|C|$ such products, and so takes $O(|C|b)$ time.
So the full runtime is bounded above by $|C|b2^{m^*+1}$.
Since $m^* = \theta(\log (k n^k))$ and $b = \theta(\log(n))$, the runtime is bounded by $2|C|bk n^k = O(\log(n)n^k) = O(n^{k+1})$.

% (I tried to solve it myself for a few hours first.
% I also spent a while trying to rework parts of his result but in the end I decided his construction was quite nice and mostly replicated that.)
% I read enough  get the gist of how he used a similar recursion to in Satvitch's theorem to get to a $\log(n)^2$ space bound, and to get the idea of searching for a way to extend the PRG as to not change the results for a particular Turing machine at hand.
% I didn't read his analysis of the algorithm in detail.)

% Say M is a log-space probabilistic TM.

% Then it uses $c \log(n)$ space, and runs in $2^{O(c \log(n))} = O(n^k)$ for some $k$.

% Given x of size n:



% For every string y of length $n^k$, run M on (x, y).
% Count the number of acceptances.

% Return true if most of the runs accepted.

% We need to track:

% 1. The current string y (k log(n) bits)

% 2. The number of acceptances (k log(n) bits)

% 3. Space for the current run of M (c log(n) bits).

% Altogether this is O(log(n)) bits.

% If this is true, then $\BPL \subseteq \textsf{SPACE}[log(n)] \subseteq \P$, solving both (a) and (b).

% \textbf{Is this wrong somehow?}

% Note to self: I think that if my TM uses $\log(n)$ space, I can get one with $1 - 2^{-f(n)}$ error in $\log(n) \log [f(n)]$ space.

% /newpage

% Ideas:

% Given input x, consider the configuration graph G, of size <= kn^k.
% Some of the transitions are deterministic, some occur with probability 1/2.
% For each path, we can get its probability by just multiplying the probabilities for each transition.
% The probability $p_\text{acc}$ of acceptance is the sum of the probability on each path from $c_\text{init}$ to $c_\text{accept}$.
% Either $p_\text{acc} > 2/3$ or it's $ < 1/3$.



% Algorithm to find number of paths from S -> T in m steps:
% Count = 0
% For c in C:
%   For k = 1, ..., m-1:
%     a = num paths S -> c in k
% 	b = num paths S -> c in m-k


















\newpage
\section*{Problem 5: Advice Removal (2 points)}

\subsection*{Question}
Assume $s : \N \rightarrow \N$ is a time constructible function. Recall $\mathsf{P}/s(n)$ is the class of languages decidable by a polynomial time algorithm with $s(n)$ advice. 

Prove that if $\textsf{SAT} \in \mathsf{P}/s(n)$ then $\textsf{SAT}$ can be solved in $2^{O(s(n))}\cdot \poly(n)$ time. That is, there is an algorithm running in $2^{O(s(n))}\cdot \poly(n)$ time which, given any formula $\phi$ of size $n$, outputs a satisfying assignment to $\phi$ when one exists.

(This result is interesting, in part because it is a major open problem whether $\textsf{SAT} \in \mathsf{P}/\mathsf{poly}$ implies $\P = \NP$ or not!)

\emph{Hint: Try using the self-reducibility of $\mathsf{SAT}$.}

\subsection*{Answer}

Say $\textsf{SAT} \in \P/s(n)$, witnessed by an algorithm $A(\phi, y)$ which decides whether $\phi$ is satisfiable using advice $y$, and which runs in $\poly(n)$ time.

Here is a $2^{O(s(n))} \poly(n)$ time algorithm for deciding if a given formula $\phi$ of description length $n$ is satisfiable.
The idea is to use a loop that runs up to $2^{s(n)}$ iterations to try solving SAT with each of the $2^{s(n)}$ pieces of advice $y \in \{0, 1\}^n$.
Within each iteration of the loop, an assignment $\vec{x}$ is constructed using the regular polynomial-time SEARCH-SAT procedure, assuming $A(\phi, y)$ decides SAT.
Since some $y$ won't make $A$ correctly decide SAT, the algorithm then checks that $\vec{x}$ is actually a valid assignment before returning.
Since there is at least one piece of advice $y$ that makes $A$ decide SAT, it is guaranteed to find a satisfying assignment on one iteration if one exists.

\begin{algorithm}{$\mathsf{SolveSAT}(\phi)$}
\begin{algorithmic}[1]
	\FOR{$y \in \{0, 1\}^{s(n)}$}
		\STATE $\vec{x} \gets []$
		\FOR{$i = 1, \dots, n$}
			\STATE $\phi_i \gets \phi$ substituted with $X_j = \vec{x}[j]$ for each $j= 1, \dots, i-1$.
			\STATE $\phi_i \gets \phi_i$ substituted with $X_i = 0$
			\STATE $b \gets A(\phi_i, y)$
			\STATE If $b$ then $x_i \gets 0$ else $x_i \gets 1$.
			\STATE $\vec{x} \gets \text{append}(\vec{x}, x_i)$
		\ENDFOR
		\IF{$\phi(\vec{x})$}
			\RETURN 1 \COMMENT{If $\vec{x}$ is a satisfying assignment, return 1.}
		\ENDIF
	\ENDFOR
	\RETURN 0
\end{algorithmic}
\end{algorithm}

% For each of the $2^{s(n)}$ bit strings $y$ of length $s(n)$, the algorithm will do the following.

% , run $A(\phi_1, y)$, where $\phi_1$ is $\phi$ substituted with $X_1 = 1$.
% If for any $y$, $A$ says that $\phi_1$ is satisfiable, set $x_1 = 1$; else set $x_1 = 0$.

% Now for each $y$, check $A(\phi_2, y)$ where $\phi_2$ is $\phi$, with $X_1 = x_1$ and $X_2 = 1$ substituted.  Continue this process through $\phi_n$ to get an assignment $x_1, x_2, \dots, x_n$.

% Now, check if $\phi(x_1, x_2, \dots, x_n) = 1$, and accept if so, and reject if not.

% This process certainly decides $\phi \in \mathsf{SAT}$.
% And each of the $n$ steps require $2^{O(s(n))} \poly(n)$ time, so the whole algorithm uses $2^{O(s(n))} \poly(n)$ time.

\newpage
\section*{Problem 6: Pairwise Independence (6 Points, 2 for each sub-problem)}  

\subsection*{Question}
In this problem, we will show how to generate ``pseudo-random bits'' with a nice property. These are sometimes useful for derandomizing algorithms. We will use them to prove other theorems in the class.

A set of discrete random variables $X_1, X_2, X_3, \ldots, X_n$ is {\em pairwise independent} if for every $i \neq j$,
\[\Pr[ X_i = a \mid X_j = b ] = \Pr[ X_i = a]\] for every $a,b$ in the range of the variables.
$X_i$ is {\em unbiased} if each element of the range of $X_i$ is equally likely.

\begin{enumerate}

\item[(a)] {\bf Generating Pairwise Independent Bits.} Let $V_n$ be the set of all $n$-bit vectors excluding the all-$0$ vector. For every $v \in V_n$,
define $X_v (r) = \langle v, r \rangle$ where $r$ is any $n$-bit vector and $\langle,\rangle$ is inner product modulo $2$. Picking $r$ uniformly at random, the $X_v$'s are well-defined random variables. \\
{\bf Show that the $X_v$'s are unbiased and pairwise independent.} 

Note we use $n$ random bits to select $r$, and obtain $2^n - 1$ pairwise independent bits $X_v$! (One can think of the above process as a ``pairwise independent" pseudorandom generator that takes an $n$-bit seed and produces $2^n-1$ bits.)
Also note if we include the all-$0$ vector in $V_n$, we would lose the unbiased property, but the $X_i$'s would still be pairwise independent. \end{enumerate} 
\begin{enumerate}
\item[(b)] {\bf Pairwise Independent Hashing.} Let $p$ be prime and $\F_p$ be the field of integers modulo $p$. For $a, b$ independently
and uniformly chosen at random from $\F_p$, let $Y_i = a\cdot i+b $.\\
{\bf  Show that $\{Y_i \mid i\in \F_p \}$ are unbiased and pairwise independent.}

{\em Hint: For any fixed values $Y_i$ and $Y_j$, the equations $Y_i = ai+b $ and $Y_j = aj+b $ can be uniquely solved for $a$ and $b$.}  \end{enumerate} 
\begin{enumerate}
\item[(c)] We will work over $\F_2$. Let $V_n$ be the set of all $n$-bit vectors excluding the all-$0$ vector. 
Letting $M$ be a uniformly randomly chosen Boolean $m \times n$ matrix, define the random variable $X_v = Mv$. \\
{\bf Show that the $X_v$'s are unbiased and pairwise independent.}

Define the function family $f_M (v) = Mv$, as we vary over all matrices $M$. \\
{\bf Show for all $v \neq v'$, when we pick a function $f$ uniformly at random from this
family, $\Pr_f[ f(v)=a \wedge f(v')=b]$ is the same for all $a$ and $b$.}

Such a family of functions is called a {\em pairwise independent family of hash
functions}. They are very useful! 
\end{enumerate} 

\subsection*{Answer to (a)}

\textbf{Verifying unbiasedness.}
Fix $v \in V_n$.
Let $I := \{i \in \mathbb{Z}_n : v_i = 1\}$.
Observe $$X_v(r) = \sum_{i \in I} r_i \mod 2$$
\noindent I proceed by induction on $|I|$.

If $|I| = 1$, let $i$ be the unique member of $I$.
Then $X_v(r) = r_i$, which is an unbiased random bit.

Now say we know that for any set $J$ so that $|J| = m$, $\sum_{j \in J} r_j \mod 2$ is unbiased.
Consider a set $I$ with $|I| = m + 1$.
Set $i^* := \sup I$ and set $J = I \setminus \{i^*\}$.
Then $Y(r) := \sum_{j \in J} r_j \mod 2$ is an unbiased bit.
Observe $$X_v(r) = \sum_{i \in I} r_i \mod 2 = r_{i^*} + Y(r) \mod 2$$
Here, $r_{i^*}$ and $Y(r)$ are independent unbiased bits.
It is straightforward to verify from this that $X_v(r)$ is itself an unbiased random bit, by enumerating the probability table of the 4 joint assignments to $X_v(r)$ and $Y(r)$.

\smallskip
\noindent \textbf{Verifying pairwise independence.}
Let $v^1, v^2 \in V_n$, with $v^1 \neq v^2$.
Let $I^1 := \{i \in \mathbb{Z}_n : v^1_i = 1\}$ and let
$I^2 := \{i \in \mathbb{Z}_n : v^2_i = 1\}$.

% Observe 
% $$
% X_{v^1}(r) = X_{v^2}(r) - \sum_{i \in I^2 \setminus I^1}{r_i} + \sum_{i \in I^1 \setminus I^2} r_i \mod 2
% $$
% The inductive argument used to show unbiasedness suffices to show that 

Let $I^* := I^1 \Delta I^2$ ($= I^1 \cup I^2 \setminus (I^1 \cap I^2)$).
I proceed by induction on $|I^*|$.

If $|I^*| = 1$, WLOG say $v^1_i = 1$ but $v^2_i = 0$ (otherwise swap $v^1$ and $v^2$).
Then $X_{v^1}(r) = X_{v^2}(r) + r_i \mod 2$, with $r_i$ independent from $X_{v^2}(r)$ (which is a sum of $r_j$ for $j \neq i$).
Then $X_{v^1}(r)$ is uniform on $\{0, 1\}$, conditional either on $X_{v^2}(r) = 1$ or $X_{v^2}(r) = 0$,
so $X_{v^1}(r)$ and $X_{v^2}(r)$ are independent.

Now say we know that if $|I^*| = m$, $X_{v^1}(r)$ and $X_{v^2}(r)$ are independent.
Say we have $|I^*| = m + 1$.
WLOG say $i$ is some index so $v^1_i = 1$ but $v^2_i = 0$ (if this doesn't exist, swap $v^1$ and $v^2$).
Let $\bar{v}$ be $v^1$ but with index $i$ set to 0.
Then $$\bar{x}(r) := \langle r, \bar{v} \rangle \mod 2 = X_{v^1}(r) - r_i \mod 2$$
By the inductive hypothesis, $\bar{x}(r)$ is independent from $X_{v^2}(r)$.
And it is also certainly independent from $r_i$ since it is a sum of $r_j$ all with $j \neq i$.
Thus $X_{v^1}(r) = \bar{x}(r) + r_i \mod 2$ is a function of two values independent of $X_{v^2}(r)$, and hence is independent of $X_{v^2}(r)$.
% $P[X_{v^1}(r) = 1 | X_{v}]

\subsection*{Answer to (b)}

\textbf{Unbiasedness.}
Fix $i$ and $a$.  Observe that since $b$ is uniform over $\mathbb{Z}_p$, $ai + b$ is also uniform over $\mathbb{Z}_p$.  Thus $\mathbb{P}[Y_i = y | a] = 1/p$ for all $y$.  Thus $\mathbb{P}[Y_i = 1] = \sum_{a = 0}^{p-1}\frac{1}{p}\mathbb{P}[Y_i = y | a] = \frac{1}{p}$.

\textbf{Pairwise independence.}
Fix $i$ and $j$.
We want to show that $\mathbb{P}[Y_i = y_i | Y_j = y_j] = \frac{1}{p}$.
Because $\mathbb{Z}_p$ is a field, for any value $a$ and any value $y_j$, there is exactly one value $b$ such that $y_j = a \cdot j + b$.
Thus, the event $E_j := \{Y_j = y_j\}$ equals the event $\{(a, b) : a \in \mathbb{Z}_p \wedge b = y_j - a \cdot j\}$, an event with probability $p/p^2 = 1/p$.
Since $\mathbb{Z}_p$ is a field, the system of linearly independent equations
$$
y_i = a \cdot i + b; \qquad y_j = a \cdot j + b
$$
has a unique solution $(a^*, b^*)$.
Thus $\{Y_j = y_j \wedge Y_i = y_i\} = \{a^*, b^*\}$ which has probability $1/p^2$.
Thus $\mathbb{P}[Y_i = y_i | Y_j = y_j] = \frac{\mathbb{P}[Y_i = y_i, Y_j = y_j]}{\mathbb{P}[Y_j = y_j]} = \frac{1/p^2}{1/p} = 1/p$.
\subsection*{Answer to (c)}

\textbf{Unbiasedness.} From part (a) we know that $(Mv)_i$ is a uniform bit for each $i$, where $(Mv)_i$ is the $i$th element of vector $Mv$.  Furthermore, since $(Mv)_i$ depends on a different row of $M$ for each $i$, the collection of values $\{(Mv)_i\}_i$ is an independent collection of random variables.  Thus $Mv$ is uniformly distributed on the space of binary n-dimensional vectors.

\textbf{Pairwise independence.}
Fix $v^1, v^2$.
Observe that 
\begin{multline*}
\mathbb{P}[Mv^1 = v | Mv^2] = \\
\mathbb{P}[(Mv^1)_0 = v_0 | Mv^2] \times 
\mathbb{P}[(Mv^1)_1 = v_1 | Mv^2, (Mv^1)_0] \times \dots \times \\
\mathbb{P}[(Mv^1)_{n-1} = v_{n-1} | Mv^2, (Mv^1)_{0:n-2}]
\end{multline*}

\noindent For any $i$,
$$
\mathbb{P}[(Mv^1)_i = v_i | Mv^2, (Mv^1)_{0:i-1}] =
\mathbb{P}[(Mv^1)_i = v_i | Mv^2_i]
$$
because all the values being conditioned on other than $Mv^2_i$ depend on entirely independent rows of $M$ from those that affect the value $(Mv^1)_i$.
And from part (a), we know $(Mv^1)_i$ and $(Mv^2)_i$ are independent for all $i$, so $\mathbb{P}[(Mv^1)_i = v_i | Mv^2_i] = 1/2$.
Thus $\mathbb{P}[Mv^1 = v | Mv^2] = 1/2^n = \mathbb{P}[Mv^1 = v]$.

\smallskip
\noindent \textbf{Uniformity on joint assignments.}
\vspace{-4mm}
$$
\mathbb{P}[f(v) = a \wedge f(v') = b] =
\mathbb{P}[f(v) = a]\times\mathbb{P}[f(v') = b | f(v) = a]
= \frac{1}{2^n} \times \frac{1}{2^n}
$$
where the last equality follows due to the pairwise independence and uniformity proven above.

\end{document}